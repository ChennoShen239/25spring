This is the 8th lecture for Econ 236B, should be served with [[Lecture_8.pdf]]

> [!PDF|] [[Lecture_8.pdf#page=3&selection=29,43,29,75|Lecture_8, p.3]]
> > ore gradually to the aggregate s
> 
> That's why we should focus on partial update or the Sticky expectation

> [!PDF|] [[Lecture_8.pdf#page=8&selection=227,3,227,21|Lecture_8, p.8]]
> > the “Kalman Gain.”
> 
> How much you react to the signal

> [!PDF|] [[Lecture_8.pdf#page=9&selection=106,0,143,7|Lecture_8, p.9]]
> > predictive distribution xt+1 ∼ N ˆxt+1, σ 2 t+1  , which is period t + 1 prior
> 
> Notice that since this is an AR(1) process, $w$ contributes nothing to the prediction mean.

> [!PDF|] [[Lecture_8.pdf#page=10&selection=79,20,93,1|Lecture_8, p.10]]
> > : how average period t posterior responds to the shock wt (in xt )
> 
> This is another way on how to interpret Kalman Gain.


> [!PDF|] [[Lecture_8.pdf#page=16&selection=88,6,88,32|Lecture_8, p.16]]
> > large information friction
> 
> No forecasters almost have the underlying micro data for all the information they need.

> [!PDF|] [[Lecture_8.pdf#page=19&selection=30,0,45,2|Lecture_8, p.19]]
> > xt+h − Et [xt+h] 
> 
> This is just the prediction error mentioned above


> [!PDF|red] [[Lecture_8.pdf#page=20&selection=77,0,82,1&color=red|Lecture_8, p.20]]
> > 1 − λ λ
> 
> using sticky forecast, here we have $\frac{1-\lambda}{\lambda}$ but for full information rational expectation we have $0$ here.

> [!PDF|yellow] [[Lecture_8.pdf#page=22&selection=64,0,66,21&color=yellow|Lecture_8, p.22]]
> > Map to the degree of information frictions
> 
> Just run the regression then just back out the $\lambda$ using $\beta$ you have.

> [!PDF|yellow] [[Lecture_8.pdf#page=24&selection=13,0,13,43&color=yellow|Lecture_8, p.24]]
> > Information rigidity exists for all variabl
> 
> Important 

> [!PDF|note] [[Lecture_8.pdf#page=28&selection=67,0,67,9&color=note|Lecture_8, p.28]]
> > Blue Chip
> 
> Otherwise it'll be hard to get the panel data.

You can overreaction to the individual information and the set the information to be so noisy that in the aggregate level it's indeed under-reaction.

> [!PDF|yellow] [[Lecture_8.pdf#page=41&selection=20,0,20,14&color=yellow|Lecture_8, p.41]]
> > Lucas critique
> 
> Lucas argued that economic models based on historical data often fail to predict the effects of new policies accurately. Why? Because people’s behavior changes when policies change—they adapt based on their expectations.

> [!PDF|yellow] [[Lecture_8.pdf#page=42&selection=16,0,47,2&color=yellow|Lecture_8, p.42]]
> > Ei,t−1[xt+1] + ω (xt − Ei,t−1[xt ])
> 
> Previous expectation $E_{i,t-1}[x_{t+1}]$ and some adjustment to the prediction error $\omega(x_{t}-E_{i,t-1}[x_{}])$

> [!PDF|] [[Lecture_8.pdf#page=46&selection=44,0,44,61|Lecture_8, p.46]]
> > Plausible explanations of over-reaction for transitory proces
> 
> Easy to generate the result

> [!PDF|red] [[Lecture_8.pdf#page=52&selection=15,1,17,4&color=red|Lecture_8, p.52]]
> > good future is more likely given good news, so people overweight good future given good news
> 
> You overweight the probability of good future given good news. It's the same for bad news $\to$ bad future.


> [!PDF|yellow] [[Lecture_8.pdf#page=56&selection=4,4,4,11&color=yellow|Lecture_8, p.56]]
> >  “FIRE”
> 
>  Full information rational expectation

> [!PDF|yellow] [[Lecture_8.pdf#page=57&selection=36,0,39,1&color=yellow|Lecture_8, p.57]]
> > receives a noisy signal:
> 
> You observe a signal instead of the true realization. And the diagnostic  follows

> [!PDF|yellow] [[Lecture_8.pdf#page=64&selection=0,0,0,29&color=yellow|Lecture_8, p.64]]
> > Theoretical Predictions: IRFs
> 
> Note the figure in the down-right side. The FE goes eventually negative then that's the over-confidence.


