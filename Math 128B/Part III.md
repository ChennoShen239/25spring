## Appetizer: power method
- **Power method**: the simplest iterative method for computing an eigenvector (in particular, the dominant eigenvector)
	- Iterative means that the eigenvector is constructed as a limit of a convergent sequence of vectors
- It can also be viewed as the simplest matrix-free method
    - Recall: matrix-free means that we only rely on matrix-vector multiplication (*do not necessarily need to construct the full matrix $A$*)
    - Recall: sometimes matvecs are cheaper to implement than forming the entire matrix $A$ and doing a full matvec
      * e.g., $A$ is sparse + low-rank
- In practice, one often only wants one or a few (i.e., $O(1)$) key eigenpairs
    - Important examples to follow
    - If matvec costs $O(n)$, then it is possible to achieve this in $O(n)$ time
    - Much better than a full diagonalization, which is always $O(n^3)$
  - Start with any initial vector $x \in \mathbb{R}^n$ (or $\mathbb{C}^n$).
    - Consider the sequence $Ax, A^2x, A^3x...$
    - What does this converge to?
		- Trick question: usually doesn't converge, or converges to zero vector
    - But if we 'suitably normalize' the sequence, what does it converge to? (Assume for simplicity that $A$ is diagonalizable)
        * The dominant eigenvector (if it exists), i.e., *the eigenvector whose eigenvalue has maximal absolute value*
- Indeed, suppose $A$ has a (strictly) dominant eigenpair $(\lambda_1, v_1)$ and $A$ is diagonalizable
    - In particular, $\lambda_1 \neq 0$.
    - Note that if $A$ is real, since all complex eigenvalues come in conjugate pairs, any dominant eigenvalue must also be real.
    - Then write $x = \sum_{i=1}^{n} c_i v_i$, where $v_2, \ldots, v_n$ complete $v_1$ to a basis of eigenvectors.
    - Then $A^k x = \sum_{i=1}^{n} c_i \lambda_i^k v_i$.
    - So $\lambda_1^{-k} A^k x = \sum_{i=1}^{n} c_i \left(\frac{\lambda_i}{\lambda_1}\right)^k v_i$ .
    - But for $i > 1$, $\left|\frac{\lambda_i}{\lambda_1}\right| < 1$, so $\left(\frac{\lambda_i}{\lambda_1}\right)^k \to 0$.
    - Hence $\lambda_1^{-k} A^k x \to c_1 v_1$.
    - Note that we're in trouble if $c_1 = 0$. We'll come back to this later.
-  In practice, we don't know $\lambda_1$, so we can't form the sequence $\{\lambda_1^{-k} A^k x\}_{k=0}^\infty$.
-  It is not important to know $\lambda_1$, we just need to normalize the iterates so that they don't collapse to zero or fly off to infinity.

Define 
$$u^{(k)} := \frac{A^k x}{\|A^k x\|}$$

where we adopt for now the 2-norm as our norm without including in the notation.
> I believe that here we have $x$ is arbitrarily picked, but we must have $c_{1} \neq 0$, otherwise the algorithm can't learn the perfect direction

- Note that as $k ‚Üí ‚àû$, as long as $c_1 ‚â† 0$, we have

$$u^{(k)} = \underbrace{ \frac{\sum_{i=1}^n c_i \lambda_i^k v_i}{||\sum_{i=1}^n c_i \lambda_i^k v_i||} ‚âà \frac{c_1 Œª_1^k v_1}{|c_1Œª_1^k |\|v_1\|} }_{ \text{since } \lambda_{1} \text{ is the largest} } = \underbrace{ \frac{c_1}{|c_1|}\left(\frac{Œª_1}{|Œª_1|}\right)^k  }_{ =:z^{(k)} } \frac{v_1}{\lVert v_{1} \rVert } = z^{(k)} u_1$$
where $|z^{(k)}| = 1$. Hence in the real case, $z^{(k)} = ¬±1$.

- This implies the following.
- **Theorem**: With notation as above there exists a complex unit $z^{(k)}$ (which is ¬±1 in the real case) such that $\lim_{k‚Üí‚àû} \frac{u^{(k)}}{z^{(k)}} = \frac{v_1}{\|v_1\|}$.
- This suggests an algorithm that converges to $v_1$ (up to scaling), but how to terminate?

	- Given a guess $u^{(k)}$, we want to consider a stopping criterion based on the 'relative residual norm'
$$\frac{\|Au^{(k)} - Œª_1 u^{(k)}\|}{Œª_1}.$$
	- However, we don‚Äôt know $Œª_1$, so we need to come up with an iterative guess for it‚Ä¶‚Ä¶

- How to estimate eigenvalue given eigenvector guess?
	- Note that if $Au = Œªu$, then $u ¬∑ (Au) = Œª \|u\|^2$, hence $Œª = \frac{u^\dagger Au}{\|u\|^2}$.
	- If $u$ is normalized, then we have $Œª = u ¬∑ (Au) = u^\dagger Au$.
	> this is how you estimate and update the guess of $\lambda$ 
	
	- Therefore guess $Œª_1^{(k)} = u^{(k)} ¬∑ (Au^{(k)})$, and estimate relative residual norm as
$$\frac{\|Au^{(k)} - Œª_1^{(k)} u^{(k)}\|}{Œª_1^{(k)}}.$$

- This suggests the practical algorithm (**power method**):
	- Given initial guesses $x^{(0)} ‚àà ‚Ñù^n$, maximum number of iterations $m$, ability to perform matvecs by A, tolerance $Œµ$.

	- For $k = 0, ‚Ä¶, m$:
		* Set $u^{(k)} = x^{(k)} / \|x^{(k)}\|$.
		* Set $x^{(k+1)} = Au^{(k)}$.
		* Set $Œª_1^{(k)} = u^{(k)} ¬∑ x^{(k+1)}$.
		* If $\|x^{(k+1)} - Œª_1^{(k)} u^{(k)}\| / Œª_1^{(k)} ‚â§ Œµ$:
			- BREAK.
	- Return $(Œª_1^{(k)}, u^{(k)})$.

- The power method works even if A is not diagonalizable, as long as there is a strictly dominant eigenvector
	- Proof more subtle, relies on Jordan normal form
-  Can generalize to the problem of finding $k ‚â• 1$ most dominant eigenvectors (**subspace iteration**)
## Markov Chain
- Think of a system with $n$ different states, labeled $1,\ldots,n$
	- Interested in random transitions between states at discrete time steps
	- Specifically, suppose that for each state $j$, we are given a column of probabilities $p_j := (p_{1j},\ldots,p_{nj})^\top$
	- Given that we are in state $j$ at time $t$, *$p_{ij}$ represents the probability that we move to state $i$ at time $t+1$*
	- The transition probabilities only depend on the current state (Markov property)
	- To ensure the probabilistic interpretation we must have
		-  $p_{ij} \geq 0$ for all $i,j$
		-  $\sum_{i=1}^{n} p_{ij} = 1$ for all $j$
		- Can also write $\mathbf{1}^\top p_j = 1$ for all $j$, where $\mathbf{1}$ is the vector of all ones
		> the third one is basically the matrix version of the second property
		
	- $P = (p_{ij})$ satisfying these two properties is called **a (column) stochastic matrix**
		-  Can rephrase second property concisely as $P^\top \mathbf{1} = \mathbf{1}$.
		- Can write $P \geq 0$ to indicate entry-wise non-negativity.
	- $P$ is in addition **positive** if $p_{ij} > 0$ for all $i,j$, in which case we may write $P > 0$.
		- We will assume positivity later for simplicity.

- Note that $\pi \in \mathbb{R}^n$ defines a **probability distribution** over $\{1,\ldots,n\}$ if $\pi_i \geq 0$ for all $i$ and $\sum_{i=1}^{n} \pi_i = 1$. (We can write these properties alternatively as $\pi \geq 0$ and $\mathbf{1}^\top \pi = 1.$)

- Let $X_t \in \{1,\ldots,n\}$ be the random variable denoting our state at time $t$
	- **Claim**: If $\pi = (\pi_1,\ldots,\pi_n)^\top$ is the probability distribution for $X_t$ at time $t$, then the probability distribution $\pi'$ at time $t+1$ is $P\pi$
    * Check:
      $$\begin{aligned}
        \pi'_i &= \mathbf{P}(X_{t+1} = i) \\
              &= \sum_{j=1}^{n} \mathbf{P}(X_{t+1} = i \mid X_t = j) \mathbf{P}(X_t = j) \\
              &= \sum_{j=1}^{n} p_{ij} \pi_j \\
              &= [P\pi]_i
      \end{aligned}$$

	  - Therefore the evolution of the probability distribution of our state can be understood purely in terms of repeated application of the stochastic matrix $P$
- Notice that if $\mathbf{1}^\top \pi = 1$, then
  $$\mathbf{1}^\top (P\pi) = (P^\top \mathbf{1})^\top \pi = \mathbf{1}^\top \pi = 1$$

	- Moreover if $\pi \geq 0$ (entry-wise), then $P\pi \geq 0$ as well.
	- This is good because it confirms that $P$ maps probability distributions to probability distributions, as we suspected.

- For arbitrary initial distribution $\pi$, interested in the limit
$$\pi_\infty = \lim_{k \to \infty} P^k \pi$$
	- If it exists, $\pi_\infty$ should then be a probability vector as well.
	- If $\pi_\infty$ exists, then by taking the limit of both sides of $\pi_{k+1} = P(\pi_k)$, we obtain
$$\pi_\infty = P\pi_\infty.$$
- If the limit exists, $\pi_\infty$ is called the **equilibrium / invariant distribution**.
	- Thusly led to ask: are the conditions for the power method satisfied?
	- If so, then $\pi_\infty$ is the (suitably normalized) dominant eigenvector of $P$.

- **Perron-Frobenius Theorem**: Let $P$ be a positive stochastic matrix. Then $P$ admits a dominant eigenvector with eigenvalue 1.
	- More refined versions of the theorem are available with weaker assumptions.

## Sub-application: PageRank
- The Internet....is a directed graph.

	- Again given a collection of states (webpages) indexed 1,...,n.
	- A directed graph is a set E of edges $(i,j)$ (order matters).
    * Indicates a hyperlink from page i to page j.
- A directed graph is equivalently specified as an adjacency matrix A, defined by
$$A_{ij} = 
    \begin{cases}
        1, & (i,j) ‚àà E \\
        0, & (i,j) ‚àâ E
    \end{cases}$$
- Want to rank pages according to some score.
    * Score should measure the 'centrality' of a webpage.
	    * Not really satisfactory just to measure how many pages link to it (why?).
    - PageRank offers such a score, an example of an eigenvector centrality measure.
- We will build a Markov chain (stochastic matrix) out of our adjacency matrix A.

	- In fact, example of a general recipe for producing a stochastic matrix from a nonnegative matrix.
	- Just appropriately normalize each column so that its sum is one.
	    * Assume each column has a nonzero entry, i.e., each page has a link to it.
	    * Otherwise, if there is a page that no one links to, we can throw it out.
	- Formula: define $\tilde{P}$ stochastic via $\tilde{P}^T = \text{diag}(A1)^{-1}A$, or$$\tilde{P}_{ji}=\begin{cases}
        \frac{1}{\deg^{-}(i)}, & (i,j) \in E \\
        0, & (i,j) \notin E,
    \end{cases}$$
	where $deg‚Åª(i)$ is the 'out degree' of a page.
	- **Probabilistic interpretation**: Markov chain on the webpages, at each time step pick a uniformly random hyperlink from current page and click it.
	- Not quite satisfactory yet...
		- *Power method may be very slow to converge*, as you will explore in the homework. By changing the problem slightly, we can ensure that it always converges rapidly.
- Introduce a damping factor $\tau \in (0,1)$.
    - Instead of clicking a random link, do the following:
        * With probability $\tau$, pick a uniformly random link on current page and click it.
        * With probability $1- \tau$, go to a random webpage picked uniformly from the entire set of webpages.
    - Corresponds to defining a new Markov chain$$P= \tau \tilde{P}+(1- \tau) \frac{1}{n} \mathbf{1} \mathbf{1}^T$$
	(note that $\mathbf{1} \mathbf{1}^T$ is just the matrix of all ones).

	- Note $P$ is stochastic and positive.
	- Hence (by Perron-Frobenius) can let $\pi^{(*)}$ be its dominant eigenvector, scaled so that $\pi^{(*)} \geq 0$ and $\mathbf{1}^{\top}\pi^{(*)} = 1$.
	    * $\pi_i^{(*)}$ is the PageRank score for webpage $i$.
	    * Corresponds to the fraction of time that our Markov chain spends on state $i$ in the long-time limit.
	- Can compute with power method.
- The **PageRank algorithm** is then just the power method applied to this matrix $P$, except even simpler: we don't need to normalize at each iteration (since the dominant eigenvalue is exactly 1).
	- Indeed, if we start with initial guess $\pi^{(0)}$ that is a probability distribution and then define $\pi^{(k+1)} = P\pi^{(k)}$ for all $k = 0, 1, 2, \ldots$, then it is guaranteed that $\pi^{(k)}$ is a probability distribution at every iteration, and $\lim_{k \to \infty} \pi^{(k)} = \pi^{(*)}$.

## Linear Algebra Review
- Review:
	 - eigenvalues
	 - diagonalization
	  - spectral theorem for symmetric matrices	
	  - viewing matrix factorizations as sums of rank-one matrices
	  - orthogonal matrices
	  - orthogonal projectors
	  - SVD (full, thin, compact, truncated)
	  - relation between SVD and spectral theorem
### üîπ **Eigenvalues and Eigenvectors**

Let $A \in \mathbb{R}^{n \times n}$. A scalar $\lambda \in \mathbb{C}$ is an **eigenvalue** of $A$ if there exists a nonzero vector $v \in \mathbb{C}^n$ such that:
$$Av = \lambda v$$

- $v$ is the corresponding **eigenvector**
- Equivalently: $\det(A - \lambda I) = 0$
### üîπ **Diagonalization**

A matrix $A \in \mathbb{R}^{n \times n}$ is **diagonalizable** if:
$$A = VDV^{-1}$$

- $D$ is diagonal, containing eigenvalues $\lambda_1, \dots, \lambda_n$
- $V$ has the corresponding eigenvectors as columns
- Diagonalizable $‚áî$ A has a basis of eigenvectors $‚áî$ geometric mult. = algebraic mult. for all eigenvalues
### üîπ **Spectral Theorem (Symmetric Matrices)**

Let $A \in \mathbb{R}^{n \times n}$ be **symmetric**: $A = A^\top$

Then:$$A = Q \Lambda Q^\top$$

- $Q$ is orthogonal ($Q^\top Q = I$), columns are orthonormal eigenvectors
- $\Lambda$ is diagonal with real eigenvalues
- This is **orthogonal diagonalization**
### üîπ **Matrix Factorizations as Sums of Rank-One Matrices**

Any diagonalizable $A \in \mathbb{R}^{n \times n}$ (or symmetric) can be written as:
$$A = \sum_{i=1}^n \lambda_i v_i v_i^\top$$

- $\lambda_i$ are eigenvalues    
- $v_i$ are orthonormal eigenvectors
- Each term $\lambda_i v_i v_i^\top$ is a **rank-one matrix**
### üîπ **Orthogonal Matrices**

A matrix $Q \in \mathbb{R}^{n \times n}$ is **orthogonal** if:$$Q^\top Q = I \quad \text{or} \quad Q^{-1} = Q^\top$$
- Columns (and rows) of Q form an orthonormal basis
- $\|Qx\| = \|x\|$: orthogonal matrices preserve inner products and norms
### üîπ **Orthogonal Projectors**

Let $V \in \mathbb{R}^{n \times k}$ have orthonormal columns. Then:
$$P = VV^\top$$

is the **orthogonal projector** onto the column space of $V$:
- $P^2 = P$,$P^\top = P$ (i.e. **idempotent and symmetric**)
- $Pv = v$ for $v \in \text{Col}(V)$
- $P^2 = P \Rightarrow \text{eigenvalues in } \{0, 1\}$
### üîπ **Singular Value Decomposition (SVD)**

For any $A \in \mathbb{R}^{m \times n}$, there exists a factorization:
$$A = U \Sigma V^\top$$

- $U \in \mathbb{R}^{m \times m}$, $V \in \mathbb{R}^{n \times n}$: orthogonal
- $\Sigma \in \mathbb{R}^{m \times n}$: diagonal with non-negative singular values $\sigma_1 \ge \cdots \ge \sigma_r > 0$
**Variants:**
- **Full SVD**: as above
- **Thin SVD**: $A = U_r \Sigma_r V_r^\top$, where $U_r \in \mathbb{R}^{m \times r}, \Sigma_r \in \mathbb{R}^{r \times r}, V_r \in \mathbb{R}^{n \times r}$
- **Compact SVD**: same as thin, but only stores nonzero $\sigma_i$
- **Truncated SVD**: keep top$- k<rk < r$ terms for low-rank approximation
### üîπ **Relation Between SVD and Spectral Theorem**

For a **symmetric** matrix $A \in \mathbb{R}^{n \times n}$:
- SVD and eigen-decomposition coincide:
$$A = Q \Lambda Q^\top \quad \text{(Spectral Thm)} \quad = Q |\Lambda| Q^\top \quad \text{(SVD, with } \sigma_i = |\lambda_i| \text{)}$$
For general A, we have:
- $A^\top A = V \Sigma^2 V^\top$  
- $A A^\top = U \Sigma^2 U^\top$

So SVD relates to the **spectral theorem** applied to $A^\top A$ and $AA^\top$, which are both symmetric and positive semidefinite.


## PCA

‚Ä¢ Suppose we have a set of ‚Äòdata points‚Äô $x_j \in \mathbb{R}^m$, $j = 1, \dots, n$ 
- $m$ is the dimension of the data space, $n$ is the number of data points
‚Ä¢ Goal: want to find a **subspace** of dimension $k$ (given) that ‚Äòcaptures‚Äô the data as well as possible 
- In statistics terms, subspace that ‚Äò**explains as much variance**‚Äô as possible
‚Ä¢ Assume that data has zero empirical mean, i.e., $\frac{1}{n} \sum_{i=1}^n x_i = 0$ 
- Always possible by replacing $x_i \leftarrow x_i - \frac{1}{n} \sum_{i=1}^n x_i$. 
- This is called **de-meaning the data.**
‚Ä¢ Can form data matrix $X = [x_1, \dots, x_n] \in \mathbb{R}^{m \times n}$ 
- FYI: $\frac{1}{n} XX^T = \frac{1}{n} \sum_i x_i x_i^T$ is called the *empirical covariance* matrix
‚Ä¢ Need quantitative formulation as an optimization problem 
- Want to optimize over $q_1, \dots, q_k$ (forming an orthonormal basis for best subspace)
- Let $Q = [q_1, \dots, q_k]$, so alternatively can optimize over $Q \in \mathbb{R}^{m \times k}$ such that $Q^T Q = I_k$ 
- There is not a completely standard name for a matrix satisfying $Q^T Q = I_k$, but we can say that such $Q$ is **partially orthogonal** or simply **has orthonormal columns.** 
-  Note: $P_Q := QQ^T$ is orthogonal projection onto $\text{span}(Q) = \text{col}(Q)$. 
---
Â•ΩÁöÑÔºåÊàë‰ª¨Êù•ËØÅÊòé $P_Q = QQ^T$ ÊòØÂà∞ $Q$ ÁöÑÂàóÁ©∫Èó¥ $\text{col}(Q)$ ‰∏äÁöÑÊ≠£‰∫§ÊäïÂΩ±ÁÆóÂ≠ê (Orthogonal Projection Operator)ÔºåÂÖ∂‰∏≠ $Q$ ÊòØ‰∏Ä‰∏™ $m \times k$ Áü©ÈòµÔºåÂÖ∂ÂàóÂêëÈáèÊòØÊ†áÂáÜÊ≠£‰∫§ÁöÑ (Orthonormal Columns)ÔºåÂç≥ $Q^T Q = I_k$„ÄÇ

**ËØÅÊòéÊÄùË∑Ø:**

‰∏Ä‰∏™ÁÆóÂ≠ê $P$ ÊòØÂà∞Â≠êÁ©∫Èó¥ $S$ ÁöÑÊ≠£‰∫§ÊäïÂΩ±ÔºåÈúÄË¶ÅÊª°Ë∂≥‰∏§‰∏™Ê†∏ÂøÉÊù°‰ª∂Ôºö
1.  ÂØπ‰∫é‰ªªÊÑèÂêëÈáè $x$ÔºåÊäïÂΩ±ÁªìÊûú $Px$ ÂøÖÈ°ªÂú®Â≠êÁ©∫Èó¥ $S$ ÂÜÖ„ÄÇ
2.  ÂØπ‰∫é‰ªªÊÑèÂêëÈáè $x$ÔºåÊÆãÂ∑ÆÂêëÈáè $(x - Px)$ ÂøÖÈ°ª‰∏éÂ≠êÁ©∫Èó¥ $S$ Ê≠£‰∫§ (Orthogonal)„ÄÇ

**ËØÅÊòéËøáÁ®ã:**

ËÆæ $Q \in \mathbb{R}^{m \times k}$ ‰∏î $Q^T Q = I_k$„ÄÇ‰ª§ $P_Q = QQ^T$„ÄÇÊàë‰ª¨Ë¶ÅÊäïÂΩ±Âà∞ÁöÑÂ≠êÁ©∫Èó¥ (Subspace) ÊòØ $S = \text{col}(Q)$ÔºåÂç≥ $Q$ ÁöÑÂàóÂêëÈáèÂº†ÊàêÁöÑÁ©∫Èó¥ (span)„ÄÇ

1.  **È™åËØÅÊäïÂΩ±ÁªìÊûúÂú®Â≠êÁ©∫Èó¥ÂÜÖ:**
    ÂØπ‰∫é‰ªªÊÑèÂêëÈáè $x \in \mathbb{R}^m$ÔºåÊàë‰ª¨ËÆ°ÁÆó $P_Q x = (QQ^T)x = Q(Q^T x)$„ÄÇ
    ‰ª§ $y = Q^T x$„ÄÇÁî±‰∫é $Q \in \mathbb{R}^{m \times k}$ ‰∏î $x \in \mathbb{R}^m$ÔºåÊâÄ‰ª• $y \in \mathbb{R}^k$ ÊòØ‰∏Ä‰∏™ $k$ Áª¥ÂêëÈáè„ÄÇ
    ÈÇ£‰πà $P_Q x = Qy = y_1 q_1 + y_2 q_2 + \dots + y_k q_k$ÔºåÂÖ∂‰∏≠ $q_i$ ÊòØ $Q$ ÁöÑÁ¨¨ $i$ ‰∏™ÂàóÂêëÈáèÔºå$y_i$ ÊòØÂêëÈáè $y$ ÁöÑÁ¨¨ $i$ ‰∏™ÂàÜÈáè„ÄÇ
    Ê†πÊçÆÂÆö‰πâÔºå$Qy$ ÊòØ $Q$ ÁöÑÂàóÂêëÈáèÁöÑÁ∫øÊÄßÁªÑÂêàÔºåÂõ†Ê≠§ $P_Q x$ ÂøÖÁÑ∂‰Ωç‰∫é $Q$ ÁöÑÂàóÁ©∫Èó¥ $\text{col}(Q)$ ‰∏≠„ÄÇ

2.  **È™åËØÅÊÆãÂ∑ÆÂêëÈáè‰∏éÂ≠êÁ©∫Èó¥Ê≠£‰∫§:**
    Êàë‰ª¨ÈúÄË¶ÅËØÅÊòéÊÆãÂ∑ÆÂêëÈáè $x - P_Q x = x - QQ^T x$ ‰∏éÂ≠êÁ©∫Èó¥ $\text{col}(Q)$ ‰∏≠ÁöÑ‰ªªÊÑèÂêëÈáè $z$ Ê≠£‰∫§„ÄÇ
    ‰∏Ä‰∏™ÂêëÈáè‰∏éÂ≠êÁ©∫Èó¥ $\text{col}(Q)$ Ê≠£‰∫§ÔºåÁ≠â‰ª∑‰∫éÂÆÉ‰∏éËØ•Â≠êÁ©∫Èó¥ÁöÑ‰∏ÄÁªÑÂü∫ (Basis) ‰∏≠ÁöÑÊâÄÊúâÂêëÈáèÈÉΩÊ≠£‰∫§„ÄÇÁî±‰∫é $Q$ ÁöÑÂàóÂêëÈáèÊòØÊ†áÂáÜÊ≠£‰∫§ÁöÑÔºåÂÆÉ‰ª¨ÊûÑÊàê‰∫Ü $\text{col}(Q)$ ÁöÑ‰∏ÄÁªÑÊ†áÂáÜÊ≠£‰∫§Âü∫„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅËØÅÊòé $x - P_Q x$ ‰∏é $Q$ ÁöÑÊâÄÊúâÂàóÂêëÈáèÈÉΩÊ≠£‰∫§Âç≥ÂèØ„ÄÇ
    ËøôÂèØ‰ª•Áî®Áü©ÈòµÂΩ¢ÂºèÁÆÄÊ¥ÅÂú∞Ë°®Á§∫‰∏∫Ôºö $Q^T (x - P_Q x) = 0$„ÄÇ
    Êàë‰ª¨Êù•ËÆ°ÁÆóÔºö
    $$\begin{align}
Q^T (x - P_Q x)  & = Q^T (x - QQ^T x)  \\
 & =Q'x - \underbrace{ Q'(Q  }_{ \text{by definition } = I_{k}  }Q'x) \\
 & =Q'x - I_{k} Q'x \\
 & = \mathbf{o}
\end{align} $$
Áî±‰∫éÁªìÊûúÊòØÈõ∂ÂêëÈáèÔºåËøôË°®ÊòéÊÆãÂ∑ÆÂêëÈáè $x - P_Q x$ ‰∏é $Q$ ÁöÑÊâÄÊúâÂàóÂêëÈáèÈÉΩÊ≠£‰∫§ÔºåÂõ†Ê≠§ÂÆÉ‰∏éÊï¥‰∏™Â≠êÁ©∫Èó¥ $\text{col}(Q)$ Ê≠£‰∫§„ÄÇ

**ÁªìËÆ∫:**
Áî±‰∫é $P_Q x = QQ^T x$ Êª°Ë∂≥Ôºö
a) $P_Q x \in \text{col}(Q)$
b) $(x - P_Q x) \perp \text{col}(Q)$
Âõ†Ê≠§Ôºå$P_Q = QQ^T$ Á°ÆÂÆûÊòØÂà∞ $Q$ ÁöÑÂàóÁ©∫Èó¥ $\text{col}(Q)$ ‰∏äÁöÑÊ≠£‰∫§ÊäïÂΩ±Áü©Èòµ„ÄÇ

**Ë°•ÂÖÖÊÄßË¥® (ÂèØÈÄâ):**
Ê≠£‰∫§ÊäïÂΩ±Áü©Èòµ $P$ ÈÄöÂ∏∏ËøòÂÖ∑Êúâ‰ª•‰∏ãÊÄßË¥®Ôºö
* **ÂπÇÁ≠âÊÄß (Idempotence):** $P^2 = P$„ÄÇ
    $P_Q^2 = (QQ^T)(QQ^T) = Q(Q^T Q)Q^T = Q(I_k)Q^T = QQ^T = P_Q$„ÄÇ
* **ÂØπÁß∞ÊÄß (Symmetry / Self-adjointness):** $P^T = P$„ÄÇ
    $P_Q^T = (QQ^T)^T = (Q^T)^T Q^T = QQ^T = P_Q$„ÄÇ
Ëøô‰∫õÊÄßË¥®‰πüÂèØ‰ª•Áî®Êù•È™åËØÅ $QQ^T$ ÊòØ‰∏Ä‰∏™Ê≠£‰∫§ÊäïÂΩ±Áü©Èòµ„ÄÇ
---

‚Ä¢ Objective function to be **minimized** is $$f(Q) := \sum_{i=1}^n \|x_i - P_Q x_i\|^2$$ 
- Measures the deviation of $x_i$ from its orthogonal projection onto the subspace (i.e., the closest point in the subspace), summed over all the data points $i$ 
- Clearly zero if $x_i \in \text{span}(Q)$ for all $i$, i.e., if all the data lies in the subspace 
- Measures deviation from this ideal scenario
‚Ä¢ Notation: the ‚Äòspan‚Äô of a matrix indicates the **span of its column**s. It is literally the same as the column space.
‚Ä¢ Compute
$$ f(Q) = \sum_{i=1}^n \|x_i - P_Q x_i\|^2 = \sum_{i=1}^n \|(I - QQ^T)x_i\|^2 $$
- Note: $(I - QQ^T)x_i$ is the $i$-th column of $(I - QQ^T)X = X - QQ^T X$, and the squared Frobenius norm is the same as the sum of the squared column norms.
- Hence $f(Q) = \|X - QQ^T X\|_F^2$.
> This is by definition of the Forbenius norm
- Let $X = U \Sigma V^T = \sum_{i=1}^r \sigma_i u_i v_i^T$ be a **compact** SVD for $X$, where $r := \text{rank}(X)$.
	- Here $U = [u_1, \dots, u_r]$ ($m \times r$)and $V = [v_1, \dots, v_r]$. ($n \times r$)
- Let $\tilde{X} = \tilde{U} \tilde{\Sigma} \tilde{V}^T = \sum_{i=1}^k \sigma_i u_i v_i^T$ be a **truncated** SVD (to rank $k$).
	- Here $\tilde{U} = [u_1, \dots, u_k]$ $m \times k$ and $\tilde{V} = [v_1, \dots, v_k]$.($n \times k$)
- Note that $\text{rank}(QQ^T X) \le k$, so we know from the ‚Äòbest rank-$k$ approximation‚Äô property of the truncated SVD that if we could choose $Q$ such that $QQ^T X = \tilde{U} \tilde{\Sigma} \tilde{V}^T$, it would be optimal.

Ê†πÊçÆ **Eckart-Young-Mirsky ÂÆöÁêÜ**ÔºåËøô‰∏™ $\tilde{X}$ ÊòØÊâÄÊúâÁß©‰∏çË∂ÖËøá $k$ ÁöÑÁü©Èòµ‰∏≠Ôºå‰∏éÂéüÁü©Èòµ X Âú®ÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞ÊÑè‰πâ‰∏ã**ÊúÄÊé•Ëøë**ÁöÑÁü©Èòµ„ÄÇ‰πüÂ∞±ÊòØËØ¥Ôºå$\lVert X-B \rVert^{2}_{F}$‚Äã ÁöÑÊúÄÂ∞èÂÄºÂú® $B = \tilde{X}$Êó∂ÂèñÂà∞ÔºåÂØπ‰∫éÊâÄÊúâ $rank(B)‚â§k$„ÄÇ

- But then just choose $Q = \tilde{U}$, so
$$ QQ^T X = \sum_{i=1}^r \tilde{U} \tilde{U}^T \sigma_i u_i v_i^T = \sum_{i=1}^k \sigma_i u_i v_i^T = \tilde{U} \tilde{\Sigma} \tilde{V}^T $$
- if $1\leq i\leq k$, then $\tilde{U}'u_{i}=e_{i}$, that is the $i$-th standard base vector. So $\tilde{U} \tilde{U}'u_{i}=\tilde{U} e_{i}=u_{i}$
- if $i>k$, then by definition we have $\tilde{U}u_{i}= 0$.


‚Ä¢ In summary, the optimal subspace is given by the span of the top $k$ left **singular vectors** of the (**de-meaned**) data matrix $X$.
- Alternatively, it is given by span of the dominant $k$ eigenvectors of the empirical covariance matrix $\frac{1}{n} XX^T$.
‚Ä¢ For a data point $x_j$, its component $u_i \cdot x_j$ in the $i$-th singular vector is called the $i$-th **principal component** of the data.
- Thus the $j$-th column of the matrix $Q^T X \in \mathbb{R}^{k \times n}$, which is $(u_1 \cdot x_j, \dots, u_k \cdot x_j)^T \in \mathbb{R}^k$ is the vector consisting of the first $k$ principal components of the $j$-th data point. 

‚Ä¢ We can use PCA for dimensionality reduction:
- The columns $y_j$ of $Y := Q^T X$ are the images of the data points $x_j$ under the linear transformation $Q^T$ mapping $\mathbb{R}^m \to \mathbb{R}^k$.
- Thus the map $Q^T$ can be thought of as ‚Äò*compressing*‚Äô our dataset.
- To *recover* the original dataset approximately from the compressed dataset, we can form $\tilde{X} = QY = QQ^T X$.
	- Recall that $Q$ was optimized to minimize the error $\| \tilde{X} - X \|_F$ of our recovery.
	- Thus, although the compression is ‚Äòlossy,‚Äô it is **as faithful as possible** given that we are compressing the data into dimension $k$.
> This $\tilde{X}$ is an orthogonal projection of $X$ on $\text{col}(Q)$


# 39 Subspace iteration

## 39.1 Basic algorithm

* Let $A \in \mathbb{R}^{n \times n}$ be diagonalizable (for simplicity). Let $\lambda_1, \dots, \lambda_n$ denote its eigenvalues (possibly repeated according to multiplicity), ordered such that $$|\lambda_1| \ge \dots \ge \underbrace{ |\lambda_k| > |\lambda_{k+1}|  }_{ \text{spectral gap}>0 }\ge \dots \ge |\lambda_n|$$
* Let $u_1, \dots, u_n$ denote a corresponding basis of eigenvectors, and let $U = [u_1, \dots, u_k] \in \mathbb{R}^{n \times k}$.
    * For now we are not assuming that $A$ is symmetric so these are not necessarily orthonormal!

* We want to find the subspace $\text{span}(U)$, in some sense.
* Let $W = [w_1, \dots, w_k] \in \mathbb{R}^{n \times k}$. (Initial guess.)
* **Theorem:** For ‚Äò*generic*‚Äô initial guess $W \in \mathbb{R}^{n \times k}$, there exists a suitable sequence of invertible matrices $C_l \in \mathbb{R}^{k \times k}$, $l = 0, 1, 2, \dots$, such that $$\lim_{l \to \infty} A^l W C_l = U$$
    * In the proof, we will say what we mean by ‚Äò*generic*‚Äô (generalizing condition for power method), and we will construct the sequence $C_l$.
* **Implication:** As $l \to \infty$, $A^l W = A(A(A(\dots AW) \dots )))$ does not literally ‚Äòconverge‚Äô to $U$, but its span ‚Äòconverges‚Äô to the $\text{span}(U)$ as a subspace.
    * However, if we literally form $A^l W$ for $l$ large, we will have issues with numerical overflow/underflow.
    * Therefore the actual practical algorithm will include a **generalization of the normalization procedure** used in the power method.

* This suggests the practical algorithm (**subspace iteration**), which we present for now without a stopping criterion:
    * Given generic initial guess $W \in \mathbb{R}^{n \times k}$, maximum number of iterations $m$, ability to perform matrix-vector multiplications by $A$.
    * Let $V$ have orthonormal columns with the same span as the columns of $W$. (This can be achieved by the Gram-Schmidt procedure, or equivalently, a rectangular QR factorization $W = VR$. The Matlab function `orth` can also be used.)
    * For $j = 0, 1, \dots, m$:
        * Set $W \leftarrow AV$. (Note that this entails $k$ matrix-vector multiplications by $A$, one for each column.)
        * Let $V$ have **orthonormal** columns with the same span as the columns of $W$. (*do the QR factorization again*)
    * Return $V$.

* Note that the bulk of the work in each iteration consists of (1) the $k$ matvecs by $A$ and (2) the orthonormalization of the $k$ columns of $W$.
    * The orthonormalization cost is $O(nk^2)$ as can verified by counting operations in Gram-Schmidt.
    * For example, if $A$ is sparse with $O(n)$ nonzero entries, then the cost of the matvecs is $O(nk)$ per iteration. If $A$ is completely dense (worst case), it is still only $O(n^2 k)$ per iteration.
    * Compare to the cost $O(n^3)$ of a complete **diagonalization**!
> The cost of subspace iteration is approximately $m \times O(nk + nk^{2})$ or $m \times O(n^{2} k + nk^{2})$.
* If the number of iterations $m$ is **sufficiently** large, then the final output $V$ will have columns forming an *orthonormal basis* for a subspace very close to $\text{span}(U)$.
* Later we will improve the algorithm to include a criterion for stopping when the algorithm has converged within a specified tolerance. However, this takes some more care than in the case of the power method.
* We will also explain later how to recover actual eigenpairs from the recovered subspace $\text{span}(V)$.

## 39.2 Convergence proof

* **Proof (of theorem):**
    * We will construct $C_l$ below and present the *genericity* condition on $W$ in ***bold italics***.
    * First let $P$ have columns $u_1, \dots, u_n$, so $$P = [U \mid V]$$ where $U=[u_1, \dots, u_k]$ and $V=[u_{k+1}, \dots, u_n]$. 
    > * (Note that the columns $u_i$ are not necessarily orthogonal since $A$ is not assumed symmetric.)
    > * Just let you know, this $P$ comes from $A = P\Lambda P^{-1}$ and we want the information (the first $k$ invariant subspace) of $A$.

    * Then $A = P \Lambda P^{-1}$, with $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$. We partition $\Lambda$ conformally with $P$:
        $$ \Lambda = \begin{pmatrix} \Lambda_1 & 0 \\ 0 & \Lambda_2 \end{pmatrix} $$
        where the top-left block $\Lambda_1 = \text{diag}(\lambda_1, \dots, \lambda_k)$ is $k \times k$, and $\Lambda_2 = \text{diag}(\lambda_{k+1}, \dots, \lambda_n)$ is $(n-k) \times (n-k)$. *Note that the diagonal matrix $\Lambda_1$ is invertible because $|\lambda_1| \ge \dots \ge |\lambda_k| > |\lambda_{k+1}| \ge 0$, implying $\lambda_1, \dots, \lambda_k$ are all non-zero.*
    * Let $Y = P^{-1} W$. We partition $Y$ conformally:
        $$ Y = \begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix} $$
        where the top block $Y_1$ is $k \times k$ and $Y_2$ is $(n-k) \times k$. We assume the *genericity* condition on $W$, which translates to: ***$Y_1$ has full rank, i.e., is invertible***.
    * In this case, let $C_l = Y_1^{-1} \Lambda_1^{-l}$. Since $Y_1$ and $\Lambda_1$ are invertible, $C_l$ is invertible. Now we examine the limit:
        $$ A^l W C_l = (P \Lambda P^{-1})^l W C_l = P \Lambda^l P^{-1} W C_l = P \Lambda^l Y C_l $$
        Substituting the partitioned forms and the definition of $C_l$:
        $$ A^l W C_l = P \begin{pmatrix} \Lambda_1^l & 0 \\ 0 & \Lambda_2^l \end{pmatrix} \begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix} (Y_1^{-1} \Lambda_1^{-l}) = P \begin{pmatrix} \Lambda_1^l Y_1 \\ \Lambda_2^l Y_2 \end{pmatrix} (Y_1^{-1} \Lambda_1^{-l}) $$
        $$ = P \begin{pmatrix} \Lambda_1^l Y_1 Y_1^{-1} \Lambda_1^{-l} \\ \Lambda_2^l Y_2 Y_1^{-1} \Lambda_1^{-l} \end{pmatrix} = P \begin{pmatrix} I_k \\ \Lambda_2^l Y_2 Y_1^{-1} \Lambda_1^{-l} \end{pmatrix} $$
    * We claim that the entire lower block $\Lambda_2^l Y_2 Y_1^{-1} \Lambda_1^{-l}$ in the last expression *converges to the zero* matrix as $l \to \infty$.
        * To see this, let $B := Y_2 Y_1^{-1}$ (a constant $(n-k) \times k$ matrix). We need to show that $M_l := \Lambda_2^l B \Lambda_1^{-l} \to 0$ as $l \to \infty$.
        * The $(i, j)$-th entry of $M_l$ is given by:
            $$ (M_l)_{ij} = (\lambda_{k+i})^l B_{ij} (\lambda_j)^{-l} = \left( \frac{\lambda_{k+i}}{\lambda_j} \right)^l B_{ij} $$
            for $i = 1, \dots, n-k$ and $j = 1, \dots, k$.
        * From the ordering of eigenvalues, we know $|\lambda_{k+i}| < |\lambda_k|$ for $i \ge 1$, and $|\lambda_j| \ge |\lambda_k|$ for $j \le k$. Therefore,
            $$ \left| \frac{\lambda_{k+i}}{\lambda_j} \right| \le \frac{|\lambda_{k+i}|}{|\lambda_k|} < 1 $$
            Since the *base* of the power has magnitude *strictly* less than 1, each entry $(M_l)_{ij} \to 0$ as $l \to \infty$. Thus, the matrix $M_l$ converges to the zero matrix.
    * Therefore, taking the limit as $l \to \infty$:
        $$ \lim_{l \to \infty} A^l W C_l = P \begin{pmatrix} I_k \\ 0 \end{pmatrix} = [U \mid V] \begin{pmatrix} I_k \\ 0 \end{pmatrix} = U I_k + V 0 = U $$
        as was to be shown.

## 39.3 Distance between subspaces

* We need to include a condition that allows us to detect when subspace iteration has converged to within a desired tolerance.
* It is tricky to do this because we want to think of subspace iteration as providing a sequence of subspaces. **How do we measure the distance between two subspaces?**
* One way to measure the distance between two subspaces is to *compute a matrix norm distance between their orthogonal projection matrices.*
* Consider two subspaces $\text{span}(V)$ and $\text{span}(\tilde{V})$, where $V \in \mathbb{R}^{n \times k}$ and $\tilde{V} \in \mathbb{R}^{n \times k}$ have orthonormal columns (i.e., $V^T V = I_k$ and $\tilde{V}^T \tilde{V} = I_k$).
    * Then $P := VV^T$ and $Q := \tilde{V}\tilde{V}^T$ are the corresponding $n \times n$ orthogonal projection matrices onto these subspaces.
    * **We will compute the squared Frobenius norm distance** $\|P - Q\|_F^2$.

* Need two facts involving the matrix **trace** $\text{Tr}[B]$ (also denoted $\text{tr}(B)$ or $\text{trace}(B)$) which is the sum of the diagonal elements of a square matrix $B$.
    * **Fact 1:** For any matrix $A$ (not necessarily square), $$\|A\|_F^2 = \text{Tr}[A^T A] = \text{Tr}[AA^T]$$
    * **Fact 2:** For any matrices $B$ ($m \times n$) and $C$ ($n \times m$), $\text{Tr}[BC] = \text{Tr}[CB]$ (Cyclic property of trace).
    * We verified in class, but it is a good exercise to try yourself!
---

### Êé®ÂØºÁü©ÈòµËøπÁöÑÊÄßË¥®

**ÂâçÊèêÂÆö‰πâ:**
* ‰∏Ä‰∏™ $m \times n$ Áü©Èòµ $A$ ÁöÑÂÖÉÁ¥†ËÆ∞‰∏∫ $A_{ij}$ ($i=1,\dots,m; j=1,\dots,n$)„ÄÇ
* Áü©Èòµ $A$ ÁöÑËΩ¨ÁΩÆ $A^T$ ÊòØ‰∏Ä‰∏™ $n \times m$ Áü©ÈòµÔºåÂÖ∂ÂÖÉÁ¥†‰∏∫ $(A^T)_{ji} = A_{ij}$„ÄÇ
* ‰∏Ä‰∏™ÊñπÈòµ $M$ (ËÆæ‰∏∫ $n \times n$) ÁöÑËøπ $\text{Tr}[M]$ ÂÆö‰πâ‰∏∫ÂÖ∂ÂØπËßíÂÖÉÁ¥†‰πãÂíåÔºö$\text{Tr}[M] = \sum_{i=1}^n M_{ii}$„ÄÇ
* ‰∏Ä‰∏™ $m \times n$ Áü©Èòµ $A$ ÁöÑÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞ (Frobenius norm) ÁöÑÂπ≥ÊñπÂÆö‰πâ‰∏∫ÊâÄÊúâÂÖÉÁ¥†Âπ≥ÊñπÂíåÔºö$\|A\|_F^2 = \sum_{i=1}^m \sum_{j=1}^n (A_{ij})^2$„ÄÇ
* Áü©Èòµ‰πòÊ≥ï $(XY)_{ik} = \sum_{j} X_{ij} Y_{jk}$„ÄÇ

---

**ÊÄßË¥® 1 (Fact 1): $\|A\|_F^2 = \text{Tr}[A^T A] = \text{Tr}[AA^T]$**

Êàë‰ª¨Ë¶ÅËØÅÊòé‰∏§‰∏™Á≠âÂºèÔºö

**(a) ËØÅÊòé $\|A\|_F^2 = \text{Tr}[A^T A]$**

ËÆæ $A$ ÊòØ‰∏Ä‰∏™ $m \times n$ ÁöÑÁü©Èòµ„ÄÇÈÇ£‰πà $A^T$ ÊòØ $n \times m$ ÁöÑÁü©ÈòµÔºå$C = A^T A$ ÊòØ‰∏Ä‰∏™ $n \times n$ ÁöÑÊñπÈòµ„ÄÇ
Êàë‰ª¨ÈúÄË¶ÅËÆ°ÁÆó $\text{Tr}[C] = \sum_{j=1}^n C_{jj}$„ÄÇ

Ê†πÊçÆÁü©Èòµ‰πòÊ≥ïÂÆö‰πâÔºå$C$ ÁöÑÂØπËßíÂÖÉÁ¥† $C_{jj}$ ‰∏∫Ôºö
$$ C_{jj} = (A^T A)_{jj} = \sum_{i=1}^m (A^T)_{ji} A_{ij} $$
Ê†πÊçÆËΩ¨ÁΩÆÁöÑÂÆö‰πâÔºå$(A^T)_{ji} = A_{ij}$„ÄÇ‰ª£ÂÖ•‰∏äÂºèÔºö
$$ C_{jj} = \sum_{i=1}^m A_{ij} A_{ij} = \sum_{i=1}^m (A_{ij})^2 $$
Ëøô‰∏™ÁªìÊûúÊòØÁü©Èòµ $A$ Á¨¨ $j$ ÂàóÊâÄÊúâÂÖÉÁ¥†ÁöÑÂπ≥ÊñπÂíå„ÄÇ

Áé∞Âú®ËÆ°ÁÆóËøπ $\text{Tr}[A^T A] = \text{Tr}[C]$Ôºö
$$ \text{Tr}[A^T A] = \sum_{j=1}^n C_{jj} = \sum_{j=1}^n \left( \sum_{i=1}^m (A_{ij})^2 \right) $$
‰∫§Êç¢Ê±ÇÂíåÈ°∫Â∫èÔºàÊúâÈôêÂíåÂèØ‰ª•Ëá™Áî±‰∫§Êç¢È°∫Â∫èÔºâÔºö
$$ \text{Tr}[A^T A] = \sum_{i=1}^m \sum_{j=1}^n (A_{ij})^2 $$
Ê†πÊçÆÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞Âπ≥ÊñπÁöÑÂÆö‰πâÔºåËøô‰∏™ÁªìÊûúÊÅ∞Â•ΩÊòØ $\|A\|_F^2$„ÄÇ
Âõ†Ê≠§Ôºå$\|A\|_F^2 = \text{Tr}[A^T A]$ ÂæóËØÅ„ÄÇ

**(b) ËØÅÊòé $\|A\|_F^2 = \text{Tr}[AA^T]$**

ËÆæ $A$ ÊòØ‰∏Ä‰∏™ $m \times n$ ÁöÑÁü©Èòµ„ÄÇÈÇ£‰πà $A^T$ ÊòØ $n \times m$ ÁöÑÁü©ÈòµÔºå$D = AA^T$ ÊòØ‰∏Ä‰∏™ $m \times m$ ÁöÑÊñπÈòµ„ÄÇ
Êàë‰ª¨ÈúÄË¶ÅËÆ°ÁÆó $\text{Tr}[D] = \sum_{i=1}^m D_{ii}$„ÄÇ

Ê†πÊçÆÁü©Èòµ‰πòÊ≥ïÂÆö‰πâÔºå$D$ ÁöÑÂØπËßíÂÖÉÁ¥† $D_{ii}$ ‰∏∫Ôºö
$$ D_{ii} = (AA^T)_{ii} = \sum_{j=1}^n A_{ij} (A^T)_{ji} $$
Ê†πÊçÆËΩ¨ÁΩÆÁöÑÂÆö‰πâÔºå$(A^T)_{ji} = A_{ij}$„ÄÇ‰ª£ÂÖ•‰∏äÂºèÔºö
$$ D_{ii} = \sum_{j=1}^n A_{ij} A_{ij} = \sum_{j=1}^n (A_{ij})^2 $$
Ëøô‰∏™ÁªìÊûúÊòØÁü©Èòµ $A$ Á¨¨ $i$ Ë°åÊâÄÊúâÂÖÉÁ¥†ÁöÑÂπ≥ÊñπÂíå„ÄÇ

Áé∞Âú®ËÆ°ÁÆóËøπ $\text{Tr}[AA^T] = \text{Tr}[D]$Ôºö
$$ \text{Tr}[AA^T] = \sum_{i=1}^m D_{ii} = \sum_{i=1}^m \left( \sum_{j=1}^n (A_{ij})^2 \right) $$
Ëøô‰∏™ÁªìÊûúÂêåÊ†∑ÊòØ $\|A\|_F^2$„ÄÇ
Âõ†Ê≠§Ôºå$\|A\|_F^2 = \text{Tr}[AA^T]$ ÂæóËØÅ„ÄÇ

ÁªìÂêà (a) Âíå (b)ÔºåÊÄßË¥® 1 ËØÅÊØï„ÄÇ

---

**ÊÄßË¥® 2 (Fact 2): $\text{Tr}[BC] = \text{Tr}[CB]$ (ËøπÁöÑÂæ™ÁéØÊÄßË¥®)**

ËÆæ $B$ ÊòØ‰∏Ä‰∏™ $m \times n$ ÁöÑÁü©ÈòµÔºå$C$ ÊòØ‰∏Ä‰∏™ $n \times m$ ÁöÑÁü©Èòµ„ÄÇÈÇ£‰πà $BC$ ÊòØ‰∏Ä‰∏™ $m \times m$ ÁöÑÊñπÈòµÔºå$CB$ ÊòØ‰∏Ä‰∏™ $n \times n$ ÁöÑÊñπÈòµ„ÄÇ‰∏§ËÄÖÈÉΩÂèØ‰ª•ËÆ°ÁÆóËøπ„ÄÇ

ËÆ°ÁÆó $\text{Tr}[BC]$Ôºö
È¶ñÂÖàËÆ°ÁÆó $BC$ ÁöÑÂØπËßíÂÖÉÁ¥† $(BC)_{ii}$ ($i=1, \dots, m$)Ôºö
$$ (BC)_{ii} = \sum_{j=1}^n B_{ij} C_{ji} $$
ÁÑ∂ÂêéËÆ°ÁÆóËøπÔºö
$$ \text{Tr}[BC] = \sum_{i=1}^m (BC)_{ii} = \sum_{i=1}^m \left( \sum_{j=1}^n B_{ij} C_{ji} \right) $$

ËÆ°ÁÆó $\text{Tr}[CB]$Ôºö
È¶ñÂÖàËÆ°ÁÆó $CB$ ÁöÑÂØπËßíÂÖÉÁ¥† $(CB)_{jj}$ ($j=1, \dots, n$)Ôºö
$$ (CB)_{jj} = \sum_{k=1}^m C_{jk} B_{kj} $$
(‰∏∫‰∫ÜÈÅøÂÖç‰∏ãÊ†áÊ∑∑Ê∑ÜÔºåËøôÈáåÁî®‰∫Ü $k$ ‰Ωú‰∏∫ÂÜÖÂ±ÇÊ±ÇÂíå‰∏ãÊ†á„ÄÇ)
ÁÑ∂ÂêéËÆ°ÁÆóËøπÔºö
$$ \text{Tr}[CB] = \sum_{j=1}^n (CB)_{jj} = \sum_{j=1}^n \left( \sum_{k=1}^m C_{jk} B_{kj} \right) $$

Áé∞Âú®ÊØîËæÉ‰∏§‰∏™ÁªìÊûúÔºö
$$ \text{Tr}[BC] = \sum_{i=1}^m \sum_{j=1}^n B_{ij} C_{ji} $$
$$ \text{Tr}[CB] = \sum_{j=1}^n \sum_{k=1}^m C_{jk} B_{kj} $$
Êàë‰ª¨ÂèØ‰ª•ÊääÁ¨¨‰∫å‰∏™ÂºèÂ≠êÈáåÁöÑ‰∏ãÊ†á $k$ Êç¢Êàê $i$ÔºàÂè™ÊòØ‰∏™ÂìëÂèòÈáèÔºâÔºö
$$ \text{Tr}[CB] = \sum_{j=1}^n \sum_{i=1}^m C_{ji} B_{ij} $$
Áî±‰∫é $B_{ij}$ Âíå $C_{ji}$ ÈÉΩÊòØÊ†áÈáèÔºåÂÆÉ‰ª¨ÁöÑ‰πòÁßØÈ°∫Â∫èÂèØ‰ª•‰∫§Êç¢Ôºö$C_{ji} B_{ij} = B_{ij} C_{ji}$„ÄÇ
$$ \text{Tr}[CB] = \sum_{j=1}^n \sum_{i=1}^m B_{ij} C_{ji} $$
ÊúÄÂêéÔºå‰∫§Êç¢Ê±ÇÂíåÈ°∫Â∫èÔºàÊúâÈôêÂíåÂèØ‰ª•Ëá™Áî±‰∫§Êç¢È°∫Â∫èÔºâÔºö
$$ \text{Tr}[CB] = \sum_{i=1}^m \sum_{j=1}^n B_{ij} C_{ji} $$
Êàë‰ª¨ÂèëÁé∞Ëøô‰∏™ÁªìÊûú‰∏é $\text{Tr}[BC]$ ÁöÑË°®ËææÂºèÂÆåÂÖ®Áõ∏Âêå„ÄÇ

Âõ†Ê≠§Ôºå$\text{Tr}[BC] = \text{Tr}[CB]$ ÂæóËØÅ„ÄÇ

---

* **Computation:**
    * First, note that $P$ and $Q$ are symmetric ($P^T=P, Q^T=Q$) and idempotent ($P^2=P, Q^2=Q$). Expand using Fact 1:
        $$\begin{align}
\|P - Q\|_F^2  & = \text{Tr}[(P - Q)^T (P - Q)]  \\
 & = \text{Tr}[(P - Q)(P - Q)]  \\
 & = \text{Tr}[P^2 - QP - PQ + Q^2] 
\end{align} $$
    * Using idempotency ($P^2=P, Q^2=Q$) and Fact 2 ($\text{Tr}[QP] = \text{Tr}[PQ]$):
        $$ \|P - Q\|_F^2 = \text{Tr}[P] + \text{Tr}[Q] - 2\text{Tr}[PQ] $$
    * Substitute $P=VV^T$ and $Q=\tilde{V}\tilde{V}^T$:
        $$ \|P - Q\|_F^2 = \text{Tr}[VV^T] + \text{Tr}[\tilde{V}\tilde{V}^T] - 2\text{Tr}[VV^T \tilde{V}\tilde{V}^T] $$
    * Now use Fact 2 (cyclic property) to rearrange the matrices within the traces. Also use $V^T V = I_k$ and $\tilde{V}^T \tilde{V} = I_k$:
    > since we assume $V$ and $\tilde{V}$ to have orthonormal columns
        $$ \text{Tr}[VV^T] = \text{Tr}[V^T V] = \text{Tr}[I_k] = k $$
        $$ \text{Tr}[\tilde{V}\tilde{V}^T] = \text{Tr}[\tilde{V}^T \tilde{V}] = \text{Tr}[I_k] = k $$
        $$ \text{Tr}[VV^T \tilde{V}\tilde{V}^T] = \text{Tr}[\tilde{V}^T (VV^T) \tilde{V}] = \text{Tr}[\underbrace{ (\tilde{V}^T V) }_{ X' } \underbrace{ (V^T \tilde{V}) }_{ X }] $$
    * Let $X = V^T \tilde{V}$ (a $k \times k$ matrix). The last trace term becomes $\text{Tr}[X^T X]$. By Fact 1, $\text{Tr}[X^T X] = \|X\|_F^2 = \|V^T \tilde{V}\|_F^2$.
    * Substituting these back:
        $$ \|P - Q\|_F^2 = k + k - 2\|V^T \tilde{V}\|_F^2 = 2k - 2\|V^T \tilde{V}\|_F^2 $$
    * It is also useful to note that $\|P\|_F^2 = \text{Tr}[P^T P] = \text{Tr}[P^2] = \text{Tr}[P] = k$. So the ‚Äúrelative distance‚Äù can be computed as:
        $$ \frac{\|P - Q\|_F}{\|P\|_F} = \frac{\sqrt{2k - 2\|V^T \tilde{V}\|_F^2}}{\sqrt{k}} = \sqrt{\frac{2(k - \|V^T \tilde{V}\|_F^2)}{k}} = \sqrt{2 \left( 1 - \frac{\|V^T \tilde{V}\|_F^2}{k} \right)} $$
* **Theorem:** If $V$ and $\tilde{V}$ are $n \times k$ matrices with orthonormal columns, and $P=VV^T$ and $Q=\tilde{V}\tilde{V}^T$ are the orthogonal projection matrices onto $\text{span}(V)$ and $\text{span}(\tilde{V})$, respectively, then
    $$ \|P - Q\|_F^2 = 2 ( k - \|V^T \tilde{V}\|_F^2 ) $$
    and
    $$ \frac{\|P - Q\|_F}{\|P\|_F} = \sqrt{2 \left( 1 - \frac{1}{k} \|V^T \tilde{V}\|_F^2 \right)} . $$
* The key point is that we can compute the Frobenius norm distance $\|P - Q\|_F$ (or its square) using the formula involving $\|V^T \tilde{V}\|_F^2$ **without ever forming the $n \times n$ projection matrices $P$ and $Q$ explicitly**. Forming $P=VV^T$ or $Q=\tilde{V}\tilde{V}^T$ directly would cost roughly $O(n^2 k)$ operations, which is very expensive for large $n$.
    * Instead, the cost is dominated by forming the $k \times k$ matrix $X = V^T \tilde{V}$.
    * This matrix multiplication takes $O(nk^2)$ floating-point operations.
    * Calculating $\|X\|_F^2$ from $X$ takes only $O(k^2)$ operations. 
    * Thus, the total cost to evaluate the distance using this formula is only $O(nk^2)$, which is much more efficient.
## 39.4 Full pseudocode

* If $V$ is our current guess (orthonormal basis matrix) in subspace iteration and $\tilde{V}$ is the basis matrix from the next iteration, then **we can use the relative Frobenius norm distance** between the orthogonal projection matrices onto their spans,
    $$ \frac{\|VV^T - \tilde{V}\tilde{V}^T\|_F}{\|VV^T\|_F} = \sqrt{2 \left( 1 - \frac{1}{k} \|V^T \tilde{V}\|_F^2 \right)} $$
    as a measure of convergence.

* This suggests the following pseudocode for subspace iteration, now including a stopping criterion:

    * **Inputs:**
        * Generic initial guess matrix $W \in \mathbb{R}^{n \times k}$.
        * Maximum number of iterations $m$.
        * Ability to perform matrix-vector multiplications by $A$.
        * Convergence tolerance $\epsilon > 0$.

    * **Initialization:**
        * Compute $V \in \mathbb{R}^{n \times k}$ such that its columns are orthonormal and $\text{span}(V) = \text{span}(W)$. (e.g., using QR decomposition of $W$, or `orth(W)`).

    * **Iteration Loop:**
        * For $j = 0, 1, \dots, m$:
            * **Power Step:** Compute $W_{next} \leftarrow AV$. (Requires $k$ matrix-vector products).
            * **Orthonormalization:** Compute $\tilde{V} \in \mathbb{R}^{n \times k}$ such that its columns are orthonormal and $\text{span}(\tilde{V}) = \text{span}(W_{next})$. (e.g., using QR decomposition of $W_{next}$, or `orth(W_{next})`).
            * **Convergence Check:** Compute the relative distance measure between the current subspace (span $V$) and the next subspace (span $\tilde{V}$):
                $$ \delta := \sqrt{2 \left( 1 - \frac{1}{k} \|V^T \tilde{V}\|_F^2 \right)} $$
                *(Note: This calculation requires $O(nk^2)$ operations, dominated by forming $V^T \tilde{V}$)*.
            * **Update:** Set $V \leftarrow \tilde{V}$.
            * **Check & Stop:** If $\delta < \epsilon$, then BREAK (exit loop).

    * **Output:** Return the final matrix $V$.

* The output $V$ contains an orthonormal basis such that $\text{span}(V)$ is approximately equal to the target dominant invariant subspace $\text{span}(U)$.

## 39.5 Recovering eigenpairs via Rayleigh-Ritz

* **We will assume in this section that $A$ is symmetric.**
* Subspace iteration (assuming for simplicity that we have fully converged it) furnishes a matrix $V \in \mathbb{R}^{n \times k}$ whose columns form an orthonormal basis for $\text{span}(U)$, the span of the dominant $k$ eigenvectors of $A$. Thus, $$\text{span}(U) = \text{span}(V)$$
    * Since $A$ is symmetric, by the **spectral theorem**, we know that the eigenvectors $u_1, \dots, u_n$ can be chosen to form an orthonormal basis. Therefore, the columns $u_1, \dots, u_k$ of $U$ (the true dominant eigenvectors) can be assumed to be orthonormal, i.e., $U^T U = I_k$.

>[!spectral theorem]
>**Ë∞±ÂÆöÁêÜÔºàÈíàÂØπÂÆûÂØπÁß∞Áü©Èòµ $A \in \mathbb{R}^{n \times n}$, $A^T = A$ÔºâÁöÑ‰∏ªË¶ÅÂÜÖÂÆπÔºö**
> 1.¬† **ÊâÄÊúâÁâπÂæÅÂÄºÈÉΩÊòØÂÆûÊï∞**: ÂØπÁß∞Áü©Èòµ $A$ ÁöÑÊâÄÊúâÁâπÂæÅÂÄº $\lambda_1, \dots, \lambda_n$ ÈÉΩÊòØÂÆûÊï∞„ÄÇ
> 2.¬† **ÂèØÂØπËßíÂåñ**: ÂØπÁß∞Áü©Èòµ $A$ ÊÄªÊòØÂèØ‰ª•Ë¢´ÂØπËßíÂåñ„ÄÇ
> 3.¬† **ÁâπÂæÅÂêëÈáèÁöÑÊ≠£‰∫§ÊÄß**:
> ¬† ¬† * ÂØπÂ∫î‰∫é**‰∏çÂêå**ÁâπÂæÅÂÄºÁöÑÁâπÂæÅÂêëÈáèÊòØ**Áõ∏‰∫íÊ≠£‰∫§ (orthogonal)** ÁöÑ„ÄÇ
> ¬† ¬† * Êõ¥Âº∫ÁöÑÊòØÔºö**Â≠òÂú®**‰∏Ä‰∏™Áî± $A$ ÁöÑÁâπÂæÅÂêëÈáèÁªÑÊàêÁöÑ $\mathbb{R}^n$ ÁöÑ**Ê†áÂáÜÊ≠£‰∫§Âü∫ (orthonormal basis)**„ÄÇËøôÊÑèÂë≥ÁùÄÊàë‰ª¨ÂèØ‰ª•ÊâæÂà∞ $n$ ‰∏™ÁâπÂæÅÂêëÈáè $q_1, \dots, q_n$ÔºåÂÆÉ‰ª¨‰∏ç‰ªÖ‰∏§‰∏§Ê≠£‰∫§ ($q_i^T q_j = 0$ for $i \neq j$)ÔºåËÄå‰∏îÊØè‰∏™ÂêëÈáèÁöÑÈïøÂ∫¶ÈÉΩ‰∏∫ 1 ($q_i^T q_i = 1$)„ÄÇ
> 4.¬† **Ê≠£‰∫§ÂØπËßíÂåñ (Orthogonal Diagonalization)**: ‰∏äËø∞ÊÄßË¥®‰øùËØÅ‰∫ÜÂØπÁß∞Áü©Èòµ $A$ ÂèØ‰ª•Ë¢´**Ê≠£‰∫§ÂØπËßíÂåñ**„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂ≠òÂú®‰∏Ä‰∏™**Ê≠£‰∫§Áü©Èòµ (orthogonal matrix)** $Q$ Âíå‰∏Ä‰∏™**ÂÆûÂØπËßíÁü©Èòµ (real diagonal matrix)** $\Lambda$Ôºå‰ΩøÂæóÔºö
> 
> ¬† ¬† $$ A = Q \Lambda Q^T $$
> 
> ¬† ¬† ÂÖ∂‰∏≠Ôºö
> ¬† ¬† * $Q$ ÊòØ‰∏Ä‰∏™ $n \times n$ ÁöÑÊ≠£‰∫§Áü©Èòµ ($Q^T Q = QQ^T = I_n$)„ÄÇ$Q$ ÁöÑÂàóÂêëÈáè $(q_1, \dots, q_n)$ Â∞±ÊòØ $A$ ÁöÑÈÇ£ÁªÑÊ†áÂáÜÊ≠£‰∫§ÁöÑÁâπÂæÅÂêëÈáè„ÄÇ
> ¬† ¬† * $\Lambda$ ÊòØ‰∏Ä‰∏™ $n \times n$ ÁöÑÂØπËßíÁü©ÈòµÔºåÂÖ∂ÂØπËßíÁ∫ø‰∏äÁöÑÂÖÉÁ¥† $(\lambda_1, \dots, \lambda_n)$ ÊòØ $A$ ÁöÑÂØπÂ∫îÁâπÂæÅÂÄº„ÄÇ
> 

* However, the subspace iteration algorithm itself only returns the basis $V$; we have not actually recovered the **eigenpairs** (eigenvalues $\lambda_j$ and eigenvectors $u_j$).
    * There is some ambiguity in the task of recovering ‚ÄúTHE‚Äù specific eigenvectors $u_1, \dots, u_k$ that form $U$. 
    * This is due to the arbitrary sign (or complex phase, though real symmetric matrices have real eigenvectors) of eigenvectors and potential non-uniqueness if there are repeated eigenvalues within the dominant group $\lambda_1, \dots, \lambda_k$. 
    * Therefore, we cannot hope to recover the exact matrix $U$ that we might have defined theoretically.
* Instead, our goal is to recover:
    * The dominant eigenvalues $\lambda_1, \dots, \lambda_k$ (perhaps up to permutation if some are equal).
    * A corresponding set of orthonormal eigenvectors $q_1, \dots, q_k$ such that $\text{span}(q_1, \dots, q_k) = \text{span}(U)$ and they satisfy the eigenvector equations $Aq_j = \lambda_j q_j$ for $j = 1, \dots, k$.
* It is useful to notice that the eigenvector equations $Au_j = \lambda_j u_j$ for $j = 1, \dots, k$, can be repackaged in the single matrix equation $AU = U \Lambda_1$, where $\Lambda_1 = \text{diag}(\lambda_1, \dots, \lambda_k)$ is the $k \times k$ diagonal matrix of the dominant eigenvalues (as used in the convergence proof).
* **Therefore, similarly, we seek an $n \times k$ matrix $Q = [q_1, \dots, q_k]$ with orthonormal columns ($Q^T Q = I_k$) such that $AQ = Q\Lambda_1$.**
* The **Rayleigh-Ritz procedure**,(ÁëûÂà©-ÈáåÂÖπ) applied using the basis $V$ obtained from subspace iteration, provides a way to achieve this:
    * **Step 1:** Form the $k \times k$ matrix $\tilde{A} = V^T A V$. Since $A$ is symmetric, $\tilde{A}$ is also symmetric. ($\tilde{A}^T = (V^T A V)^T = V^T A^T (V^T)^T = V^T A V = \tilde{A}$).
>[!Note]
>What's this $\tilde{A}$
>1. ËÆ© $V$ ÁöÑÂêÑ‰∏™ÂàóÂêëÈáè‰Ωú‰∏∫$S =\text{span}(V)$ ÁöÑ‰∏Ä‰∏™orthonormal basis
>2. for all $x \in S$, we have $x=Vy,y\in \mathbb{R}^{k}$ ÂÖ∂‰∏≠ $y=V'x$ ‰∏∫ $x$ ÁöÑÂùêÊ†á 
>3. ÂΩìÊàë‰ª¨ËÆ©$A$ ‰ΩúÁî®Âú®$x$‰∏äÊó∂ÔºåÂàô$Ax$ ÂèØËÉΩÂπ∂‰∏çÂú® $S$
>4. ‰∫éÊòØÊàë‰ª¨ÊÉ≥Âà∞ÂéªÊâæ$Ax$ Âú® $S$ ‰∏äÁöÑ projection
>5. Ê≠§Êó∂ÂùêÊ†á‰∏∫ $$y_{proj} = V'(P_{V}Ax) = V'(VV'Ax) = V'Ax$$
>6. Â∏¶ÂÖ• $x= Vy$, ÂàôÊúâ $y_{proj} = V'AVy =\tilde{A}y$
>7. Êç¢Âè•ËØùËØ¥Ôºå$\tilde{A}$ Â∞±ÊòØÂéüÊò†Â∞Ñ $A$ Âú®$S$ ‰∏≠ÁöÑ‰ª£Ë°®

 **Step 2:** Perform a complete eigen-decomposition (diagonalization) of the small $k \times k$ symmetric matrix $\tilde{A}$: 
	    * Find a $k \times k$ orthogonal matrix $\tilde{U}$ ($\tilde{U}^T \tilde{U} = I_k$) and a $k \times k$ diagonal matrix $\tilde{\Lambda}$ such that $\tilde{A} = \tilde{U} \tilde{\Lambda} \tilde{U}^T$. 
	    * The diagonal entries of $\tilde{\Lambda}$ are the eigenvalues of $\tilde{A}$ (called Ritz values), and the columns of $\tilde{U}$ are the corresponding orthonormal eigenvectors.
        * **Such a decomposition exists by the spectral theorem for symmetric matrices.**
	        * Computing it involves solving a standard $k \times k$ eigenvalue problem, which typically costs $O(k^3)$ operations (*independent of the original large dimension $n$*).
- **Step 3:** Construct the approximate eigenvectors of $A$ (called Ritz vectors) as $Q = V \tilde{U}$.
>[!note]
>Êàë‰ª¨Âú®ËøôÈáårecover $Q = V \tilde{U}$‰ªéËÄåËææÂà∞Â∞Ü$u_{i} \in\mathbb{R}^{k}$ ËΩ¨Êç¢ÂõûÂéüÂßãÁöÑ $\mathbb{R}^{n}$.
* There are two claims about this procedure:
    * **Claim 1:** The diagonal matrix of eigenvalues $\tilde{\Lambda}$ obtained from $\tilde{A}$ is equal to the diagonal matrix $\Lambda_1$ containing the **true dominant eigenvalues** of $A$. ($\tilde{\Lambda} = \Lambda_1$, perhaps up to permutation if eigenvalues are repeated).
    * **Claim 2:** The matrix $Q = V \tilde{U}$ constructed has orthonormal columns ($Q^T Q = I_k$) 
	    * and satisfies the desired matrix eigenvector equation $AQ = Q\Lambda_1$ (using the true $\Lambda_1$, which by Claim 1 equals $\tilde{\Lambda}$).

* **Proof of Claims:** We begin with a sub-claim relating the computed basis $V$ and the true basis $U$.
    * **Sub-claim:** $V = U O$ for some $k \times k$ orthogonal matrix $O$. Specifically, we can show $O = U^T V$ works.
        * **Proof that $V=U(U^T V)$:** Let $O=U^T V$. We compute $UO = U(U^T V) = (UU^T)V$. Since $A$ is symmetric, $U$ has orthonormal columns ($U^T U = I_k$), so $P_U = UU^T$ is the orthogonal projection matrix onto $\text{span}(U)$. Since subspace iteration converged fully, we have $\text{span}(V) = \text{span}(U)$. Projecting the columns of $V$ (which are already in $\text{span}(U)$) onto $\text{span}(U)$ leaves them unchanged. Therefore, $P_U V = (UU^T)V = V$. Thus, $V = UO$.
        * **Proof that $O=U^T V$ is orthogonal:** We compute $O^T O = (U^T V)^T (U^T V) = (V^T U^{TT}) (U^T V) = V^T U U^T V$. Again, since $UU^T = P_U$ and $P_U V = V$, we have $O^T O = V^T (UU^T V) = V^T (P_U V) = V^T V$. Since the columns of $V$ are orthonormal (output of subspace iteration), $V^T V = I_k$. Thus, $O^T O = I_k$, which means $O$ is an orthogonal matrix.
        * This completes the proof of the sub-claim ($V=UO$ with $O$ orthogonal).
* **Proof of Claim 1 ($\tilde{\Lambda} = \Lambda_1$):**
    * Using the sub-claim $V=UO$, we substitute into the definition of $\tilde{A}$:
        $$ \tilde{A} = V^T A V = (UO)^T A (UO) = O^T U^T A U O $$
    * Now use the matrix eigenvector equation $AU = U\Lambda_1$:
        $$ \tilde{A} = O^T U^T (U \Lambda_1) O = O^T (U^T U) \Lambda_1 O $$
    * Since $U^T U = I_k$:
        $$ \tilde{A} = O^T I_k \Lambda_1 O = O^T \Lambda_1 O $$
    * The equation $\tilde{A} = O^T \Lambda_1 O$ shows that $\tilde{A}$ is related to the diagonal matrix $\Lambda_1$ by an **orthogonal similarity transformation** (since $O$ is orthogonal, $O^T = O^{-1}$). This means that $\tilde{A}$ and $\Lambda_1$ have the same eigenvalues. The eigenvalues of $\Lambda_1$ are $\lambda_1, \dots, \lambda_k$. The eigenvalues of $\tilde{A}$ are the diagonal entries of $\tilde{\Lambda}$ from the eigendecomposition $\tilde{A} = \tilde{U} \tilde{\Lambda} \tilde{U}^T$. Therefore, the diagonal entries of $\tilde{\Lambda}$ must be $\lambda_1, \dots, \lambda_k$, possibly in a different order if some $\lambda_j$'s are equal. Assuming a consistent ordering (e.g., non-increasing), we have $\tilde{\Lambda} = \Lambda_1$. Claim 1 holds.
* **Proof of Claim 2 ($AQ = Q\Lambda_1$ and $Q^T Q = I_k$):**
    * First, check that $Q=V\tilde{U}$ has orthonormal columns:
        $$ Q^T Q = (V\tilde{U})^T (V\tilde{U}) = (\tilde{U}^T V^T) (V \tilde{U}) = \tilde{U}^T (V^T V) \tilde{U} $$
        Since $V^T V = I_k$ and $\tilde{U}$ is orthogonal ($\tilde{U}^T \tilde{U} = I_k$):
        $$ Q^T Q = \tilde{U}^T I_k \tilde{U} = \tilde{U}^T \tilde{U} = I_k $$
        So $Q$ indeed has orthonormal columns.
    * Now, we verify $AQ = Q\Lambda_1$. Start with $AQ$ and substitute $Q = V\tilde{U}$:
        $$ AQ = A(V\tilde{U}) $$
    * Substitute $V=UO$:
        $$ AQ = A(UO\tilde{U}) = (AU)O\tilde{U} $$
    * Use $AU = U\Lambda_1$:
        $$ AQ = (U\Lambda_1)O\tilde{U} = U\Lambda_1 O\tilde{U} $$
    * Insert $I_k = OO^T$ (since $O$ is orthogonal):
        $$ AQ = U (OO^T) \Lambda_1 O \tilde{U} = (UO) (O^T \Lambda_1 O) \tilde{U} $$
    * Use $UO = V$ and the earlier result $O^T \Lambda_1 O = \tilde{A}$:
        $$ AQ = V \tilde{A} \tilde{U} $$
    * Use the eigendecomposition $\tilde{A} = \tilde{U} \tilde{\Lambda} \tilde{U}^T$:
        $$ AQ = V (\tilde{U} \tilde{\Lambda} \tilde{U}^T) \tilde{U} = (V \tilde{U}) \tilde{\Lambda} (\tilde{U}^T \tilde{U}) $$
    * Use $V\tilde{U}=Q$ and $\tilde{U}^T \tilde{U} = I_k$:
        $$ AQ = Q \tilde{\Lambda} I_k = Q \tilde{\Lambda} $$
    * Finally, use Claim 1 ($\tilde{\Lambda} = \Lambda_1$):
        $$ AQ = Q \Lambda_1 $$
    * This establishes Claim 2.


# 40 ÈöèÊú∫ SVD (Randomized SVD)

* ÂÅáËÆæÊàë‰ª¨Êúâ‰∏Ä‰∏™Áü©Èòµ $A \in \mathbb{R}^{m \times n}$ÔºåÂπ∂‰∏îÂÅáÂÆö $m \ge n$„ÄÇ
* Áü©Èòµ $A$ ÁöÑÂÖ®ÈÉ®‰ø°ÊÅØÂåÖÂê´Âú®ÂÖ∂**ÁªÜÂ•áÂºÇÂÄºÂàÜËß£ (thin SVD)** ‰∏≠Ôºö$$A = \hat{U} \hat{\Sigma} \hat{V}^T$$ÔºåÂÖ∂‰∏≠ $\hat{U} \in \mathbb{R}^{m \times n}$ ÂÖ∑ÊúâÊ†áÂáÜÊ≠£‰∫§ÂàóÔºå$\hat{\Sigma} \in \mathbb{R}^{n \times n}$ ÊòØÂØπËßíÁü©ÈòµÔºàÂåÖÂê´Â•áÂºÇÂÄº $\sigma_1 \ge \dots \ge \sigma_n \ge 0$ÔºâÔºå$\hat{V} \in \mathbb{R}^{n \times n}$ ÊòØÊ≠£‰∫§Áü©Èòµ„ÄÇ
* ÂÅáËÆæÊàë‰ª¨ÊÉ≥Ë¶ÅËÆ°ÁÆó $A$ ÁöÑ**Áß© $k$ Êà™Êñ≠ SVD (rank-k truncated SVD)** $$A_k := U \Sigma V^T$$ÔºåÂÖ∂‰∏≠ $U = \hat{U}_{:, 1:k} \in \mathbb{R}^{m \times k}$ Áî±Ââç $k$ ‰∏™Â∑¶Â•áÂºÇÂêëÈáèÁªÑÊàêÔºå$\Sigma = \text{diag}(\sigma_1, \dots, \sigma_k) \in \mathbb{R}^{k \times k}$ ÂåÖÂê´Ââç $k$ ‰∏™ÊúÄÂ§ßÁöÑÂ•áÂºÇÂÄºÔºåËÄå $V = \hat{V}_{:, 1:k} \in \mathbb{R}^{n \times k}$ Áî±Ââç $k$ ‰∏™Âè≥Â•áÂºÇÂêëÈáèÁªÑÊàê„ÄÇ

* Ê≥®ÊÑèÔºå$A_k$ ÂèØ‰ª•ÈÄöËøáÂàÜÂà´Â∫îÁî®**Â≠êÁ©∫Èó¥Ëø≠‰ª£ + ÁëûÂà©-ÈáåÂÖπ**ËøáÁ®ã*‰∏§Ê¨°*Êù•ËÆ°ÁÆóÔºö
    * Êàë‰ª¨Áü•ÈÅì $AA^T = \hat{U} \hat{\Sigma}^2 \hat{U}^T$ ÁöÑ $k$ ‰∏™‰∏ªÂØºÁâπÂæÅÂêëÈáèÊûÑÊàê‰∫ÜÂâç $k$ ‰∏™Â∑¶Â•áÂºÇÂêëÈáè $U$„ÄÇÔºàÂØπÂ∫îÁöÑÁâπÂæÅÂÄºÊòØÂâç $k$ ‰∏™Â•áÂºÇÂÄºÁöÑÂπ≥Êñπ $\sigma_1^2, \dots, \sigma_k^2$Ôºâ„ÄÇ
    * ÂêåÊó∂Ôºå$A^T A = \hat{V} \hat{\Sigma}^2 \hat{V}^T$ ÁöÑ $k$ ‰∏™‰∏ªÂØºÁâπÂæÅÂêëÈáèÊûÑÊàê‰∫ÜÂâç $k$ ‰∏™Âè≥Â•áÂºÇÂêëÈáè $V$„ÄÇÔºàÂØπÂ∫îÁöÑÁâπÂæÅÂÄºÂêåÊ†∑ÊòØÂâç $k$ ‰∏™Â•áÂºÇÂÄºÁöÑÂπ≥Êñπ $\sigma_1^2, \dots, \sigma_k^2$Ôºâ„ÄÇ
    * Âõ†Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•ÂàÜÂà´ÂØπ $AA^T$ Âíå $A^T A$ Â∫îÁî®Â≠êÁ©∫Èó¥Ëø≠‰ª£ÔºàÂØªÊâæ $k$ ‰∏™‰∏ªÂØºÁâπÂæÅÂêëÈáèÔºâÔºå‰ª•Êé®ÂØºÂá∫Ââç $k$ ‰∏™Â∑¶Â•áÂºÇÂêëÈáè $U$ ÂíåÂè≥Â•áÂºÇÂêëÈáè $V$Ôºå‰ª•ÂèäÂâç $k$ ‰∏™Â•áÂºÇÂÄºÔºàÈÄöËøáÂèñÁâπÂæÅÂÄºÁöÑÂπ≥ÊñπÊ†πÂæóÂà∞ $\Sigma$Ôºâ„ÄÇ
    * **ËØ∑Ê≥®ÊÑè**ÔºöËøôÈáåÂ≠óÊØç $V$ ÁöÑÂê´‰πâ‰∏é‰πãÂâçËÆ®ËÆ∫Â≠êÁ©∫Èó¥Ëø≠‰ª£Êó∂ÁöÑ $V$ÔºàË°®Á§∫Ëø≠‰ª£‰∏≠ÁöÑÂü∫Áü©ÈòµÔºâ**‰∏çÂêå**ÔºÅÂú®Ê≠§Â§ÑÔºå$V$ ÊåáÁöÑÊòØ**Âè≥Â•áÂºÇÂêëÈáè**Áü©Èòµ„ÄÇÂæàÊä±Ê≠âÔºåÊàë‰ª¨ÊúâÊó∂‰ºöÁî®ÂÆåÁõ¥ËßÇÊòìÊáÇÁöÑÂ≠óÊØçÔºÅ

>[!What do we mean by doing Rayleigh-Ritz?]
>Let's show how to get $U$ and $\Sigma$:
>1. let $B = AA' \in \mathbb{R}^{m\times m}$,  do subspace iteration to $B$, then we can get a orthonormal basis of the span of $k$ dominant eigenvectors. The output is $V_{basisL}$
>2. Apply Rayleigh-Ritz to $V_{basisL}$, we have $\tilde{\Lambda}_{B} \approx \text{diag}(\sigma_{1}^{2},\dots \sigma_{k}^{2}),Q_{L} \approx U$

* **ÈöèÊú∫ SVD (Randomized SVD)** Êèê‰æõ‰∫Ü‰∏ÄÁßçËøë‰ººËÆ°ÁÆóÊà™Êñ≠ SVD $A_k$ ÁöÑ**Êõø‰ª£ÊñπÊ≥ï**„ÄÇ
    * ‰∏éÂ≠êÁ©∫Èó¥Ëø≠‰ª£‰∏çÂêåÔºåÈöèÊú∫ SVD **‰∏ç‰æùËµñ‰∫éËø≠‰ª£ÁÆóÊ≥ïÁöÑÊî∂Êïõ**ÔºàÈô§‰∫ÜÊúÄÂêé‰∏ÄÊ≠•‰∏≠Á±ª‰ºº‰∫éÁëûÂà©-ÈáåÂÖπËøáÁ®ãÈúÄË¶ÅÂØπ‰∏Ä‰∏™Â∞èÁü©ÈòµËøõË°åÂÆåÂÖ® SVDÔºåÂÖ∂ÊàêÊú¨Êàë‰ª¨ÈÄöÂ∏∏ËßÜ‰∏∫ÂèØÂøΩÁï•‰∏çËÆ°Ôºâ„ÄÇ
        * ‰∫ãÂÆû‰∏äÔºåÂ¶ÇÊûúÂ•áÂºÇÂÄº $\sigma_k$ Âíå $\sigma_{k+1}$ ‰πãÈó¥ÁöÑ**Èó¥Èöô (gap)** ($\sigma_k - \sigma_{k+1}$) ÂæàÂ∞èÔºåÂ≠êÁ©∫Èó¥Ëø≠‰ª£ÁöÑÊî∂ÊïõÈÄüÂ∫¶‰ºö**ÂæàÊÖ¢**ÔºÅ
    * Âè¶‰∏ÄÊñπÈù¢Ôºå‰ΩøÁî®ÈöèÊú∫ SVD Êó∂ÔºåÊàë‰ª¨ÂèØËÉΩÈúÄË¶ÅÊé•Âèó‰∏ÄÂÆöÁöÑËøë‰ººËØØÂ∑ÆÔºåÂÖ∂ËØØÂ∑ÆÁ®ãÂ∫¶ÂèñÂÜ≥‰∫é $A$ ÁöÑÂ•áÂºÇÂÄº**Ë°∞ÂáèÁöÑÈÄüÂ∫¶**ÔºàÊàë‰ª¨Á®çÂêé‰ºöÁúãÂà∞Ôºâ„ÄÇ

## 40.1 ÁÆÄÂåñÈóÆÈ¢òÔºöÁß©‰∏∫ k ÁöÑÊÉÖÂÜµ

* ‰∏∫ÁÆÄÂçïËµ∑ËßÅÔºåÂÅáËÆæ $\text{rank}(A) = k$ÔºåÊÑèÂë≥ÁùÄ $A$ ÁöÑ**ÂÄºÂüü (range)** Áª¥Â∫¶‰∏∫ $k$„ÄÇ
* ‰ª§ $Z = [z_1, \dots, z_k] \in \mathbb{R}^{n \times k}$ ÁöÑÂàóÂêëÈáèÊòØ $k$ ‰∏™‚ÄúÈöèÊú∫‚ÄùÂêëÈáèÔºàÂÖ∑‰ΩìÁªÜËäÇÁ®çÂêé‰ºöÊõ¥‰ªîÁªÜÂú∞ÈòêËø∞Ôºâ„ÄÇ
* Ê≥®ÊÑè $Az_1, \dots, Az_k \in \text{range}(A)$„ÄÇÂ¶ÇÊûú $Z$ ÊòØÈöèÊú∫ÈÄâÂèñÁöÑÔºåÊàë‰ª¨ÊúüÊúõ‰ª• 100% ÁöÑÊ¶ÇÁéáËøô‰∫õÂêëÈáèÊòØ**Á∫øÊÄßÊó†ÂÖ≥**ÁöÑÔºåÂõ†Ê≠§ÂÆÉ‰ª¨ËÉΩÂº†Êàê**Êï¥‰∏™Â≠êÁ©∫Èó¥** $\text{range}(A)$ÔºåÂç≥Êàë‰ª¨ÊúüÊúõ $\text{span}(Az_1, \dots, Az_k) = \text{span}(AZ) = \text{range}(A)$„ÄÇ

>[!range]
>The definition of range $A$ is: $$ \text{range}(A) = \left\{ y \in \mathbb{R}^{m}|y = Ax, x \in \mathbb{R}^{n}\right\} $$


* ‰∫ãÂÆû‰∏äÔºåÂú®Ëøô‰∏™ÁÆÄÂåñÊÉÖÂÜµ‰∏ãÔºå$\text{range}(A) = \text{span}(U)$„ÄÇ
    * ÂéüÂõ†ÊòØÔºåÂ¶ÇÊûú $\text{rank}(A) = k$ÔºåÈÇ£‰πàÂ•áÂºÇÂÄº $\sigma_1, \dots, \sigma_k > 0$Ôºå‰ΩÜÂØπ‰∫éÊâÄÊúâ $i > k$Ôºå$\sigma_i = 0$„ÄÇ
    * Âõ†Ê≠§ÔºåÂú® SVD ‰∏≠Ôºå$A = \sum_{i=1}^k \sigma_i u_i v_i^T$ÔºåËøô‰∏™ÂíåÂú®Á¨¨ $k$ È°πÂ∞±ÁªàÊ≠¢‰∫Ü„ÄÇ
    * Áî±Ê≠§ÂæóÂá∫ $\text{range}(A) = \text{span}(u_1, \dots, u_k) = \text{span}(U)$„ÄÇ (Ëøô‰∏™ÁªìËÆ∫Êàë‰ª¨Âú®ËØæÂ†Ç‰∏äÈ™åËØÅËøáÔºå‰πüÊòØ‰∏Ä‰∏™ÂæàÂ•ΩÁöÑÁªÉ‰π†ÔºÅ)

>[!Verification]
>The goal is to prove $\text{range}(A) = \text{span}(U),$ that is, 
>$$
\begin{align}
\text{range} (A) \subset \text{span}(U) \\
\text{span}(U) \subset \text{range}(A)
\end{align}
>$$
> then we prove them one by one:
> 1. for arbitrary $y \in \text{range}(A)$, then by definition, there exists an $x \in \mathbb{R}^{m}$ such that $y = Ax$
> 2. then we have 
>  $$
y = \sum_{i=1}^{k} \sigma_{i} u_{i}v'_{i}x$$
> then we let $c_{i} = \sigma_{i} v'_{i}x$ be a scalar, then we know $y = \sum_{i=1}^{k}c_{i}u_{i} \in \text{span}(U)$
> 3. we then prove the other side, let $z \in \text{span}(U)$ be arbitrary, then by definition, we know 
> $$
z = \sum_{i=1}^{k} d_{i}u_{i}
>$$
>since $Ax = \sum_{i=1}^{k}\sigma_{i}u_{i}v'_{i}x$, we just want find such $x \in \mathbb{R}^{n}$ such that:$$
\sum_{i=1}^{k} d_{i}u_{i} = \sum_{i=1}^{k}\sigma_{i}u_{i}v'_{i}x
>$$
>4. given $u_{1}\dots u_{k}$ are orthonormal, we have they're linear independent, so we just need 
> $$
\sigma_{i} v'_{i} x = d_{i}
>$$
>since $\sigma_{i}>0$, we know $v'_{i} x = \frac{d_{i}}{\sigma_{i}}$. Since $v_{i}$ are orthonormal, we can let $x = \sum_{i=1}^{k}\frac{d_{i}}{\sigma_{i}}v_{i}$, then it's easy to see that this $x$ satisfies our goal
>5. therefore, $\text{span}(U) \subset \text{range}(A)$

* ÊâÄ‰ª•ÔºåÂú®Ëøô‰∏™ÁÆÄÂåñÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨Êúâ $\text{span}(AZ) = \text{span}(U)$„ÄÇ
* Âõ†Ê≠§Ôºå‰ª§ $X$ ÊòØ‰∏Ä‰∏™Áü©ÈòµÔºåÂÖ∂ÂàóÂêëÈáèÊòØÊ†áÂáÜÊ≠£‰∫§ÁöÑÔºåÂπ∂‰∏îÂº†Êàê $\text{span}(AZ)$Ôºà‰πüÂ∞±ÊòØÂº†Êàê $\text{span}(U)$Ôºâ„ÄÇ

* ÈÇ£‰πàÔºå‰ΩøÁî®Â≠êÁ©∫Èó¥ $\text{span}(X)$ ÂØπÁü©Èòµ $AA^T$ Â∫îÁî®**ÁëûÂà©-ÈáåÂÖπËøáÁ®ã (Rayleigh-Ritz procedure)**ÔºåÂ∞ÜËÉΩÂæóÂà∞Ââç $k$ ‰∏™Â∑¶Â•áÂºÇÂêëÈáèÔºà‰ª•ÂèäÂâç $k$ ‰∏™Â•áÂºÇÂÄºÁöÑÂπ≥ÊñπÔºâ„ÄÇ
    * **Á¨¶Âè∑ÊèêÈÜí**ÔºöËøôÈáåÁöÑ $X$ ÊâÆÊºî‰∫ÜÊàë‰ª¨‰πãÂâçËÆ®ËÆ∫Â≠êÁ©∫Èó¥Ëø≠‰ª£Êó∂ $V$ (Ëø≠‰ª£Âü∫Áü©Èòµ) ÁöÑËßíËâ≤ÔºÅÔºàÁé∞Âú®Á¨¶Âè∑ $V$ ‰øùÁïôÁªôÂè≥Â•áÂºÇÂêëÈáè‰ΩøÁî®„ÄÇÔºâ
    * ÂÖ∑‰ΩìÊù•ËØ¥ÔºåËøôÈáåÊàë‰ª¨ÂøÖÈ°ªÊûÑÈÄ† $k \times k$ Áü©Èòµ $B := X^T AA^T X = (X^T A)(X^T A)^T$„ÄÇÊàë‰ª¨ÈÄöËøáË∞±ÂÆöÁêÜÂ∞ÜÂÖ∂ÂØπËßíÂåñ‰∏∫ $B = \tilde{U} \tilde{\Lambda} \tilde{U}^T$ ÔºàÂÖ∂‰∏≠ $\tilde{\Lambda}$ ÂåÖÂê´ $B$ ÁöÑÁâπÂæÅÂÄº $\approx \sigma_i^2$ÔºåÊåâÈùûÂ¢ûÈ°∫Â∫èÊéíÂàóÔºâ„ÄÇ
    * ÁÑ∂ÂêéÊÅ¢Â§ç $U = X \tilde{U}$ÔºåÂπ∂‰∏îÂèØ‰ª•ÈÄöËøá $\Sigma = \sqrt{\tilde{\Lambda}}$ ÔºàÂèñÂØπËßíÂÖÉÁ¥†ÁöÑÊ≠£Âπ≥ÊñπÊ†πÔºâÊÅ¢Â§çÂ•áÂºÇÂÄº„ÄÇ

>[!note]
>Note on $B$: ËøôÈáåÁöÑ $B$ is similar to the $\tilde{A}$ we talked about before, just the projection of the map $AA'$ in subspace $\text{span}(X)$

* Á±ª‰ººÁöÑËøáÁ®ãÂèØ‰ª•Â∫îÁî®‰∫é $A^T$ Êù•Êé®ÂØºÂá∫ $V$ÔºåÂõ†‰∏∫ $\text{rank}(A^T) = \text{rank}(A) = k$Ôºö
    * ‰ª§ $W = [w_1, \dots, w_k] \in \mathbb{R}^{m \times k}$ ÁöÑÂàóÂêëÈáèÊòØÈöèÊú∫ÁöÑ„ÄÇ
    * ËÆ°ÁÆó $A^T W$ Âπ∂Â∞ÜÂÖ∂ÂàóÂêëÈáèÊ†áÂáÜÊ≠£‰∫§ÂåñÂæóÂà∞ $Y$„ÄÇÔºàÂõ†Ê≠§ $\text{span}(Y) = \text{range}(A^T) = \text{span}(V)$Ôºâ„ÄÇ
    * ÊûÑÈÄ† $C := Y^T A^T A Y = (AY)^T (AY)$„ÄÇÔºàËøôÊòØÂ∞Ü $A^T A$ ÊäïÂΩ±Âà∞ $\text{span}(Y)$ ‰∏äÁöÑ $k \times k$ Áü©ÈòµÔºâ„ÄÇ
    * ÈÄöËøáË∞±ÂÆöÁêÜÂ∞ÜÂÖ∂ÂØπËßíÂåñ‰∏∫ $C = \tilde{V} \tilde{\Lambda} \tilde{V}^T$ ÔºàÊåâÈùûÂ¢ûÈ°∫Â∫èÊéíÂàóÔºåËøôÈáåÁöÑ $\tilde{\Lambda}$ ÁêÜËÆ∫‰∏äÂ∫î‰∏é‰∏äÈù¢ $B$ ÂàÜËß£ÂæóÂà∞ÁöÑÁõ∏ÂêåÔºâ„ÄÇ
    * ÊÅ¢Â§ç $V = Y \tilde{V}$„ÄÇ

* ‰ΩÜÊòØÔºåÊúâ‰∏ÄÁßçÊñπÊ≥ïÂèØ‰ª•**ÂêåÊó∂ÂÆåÊàê**Â∑¶Âè≥‰∏§ËæπÁöÑÊ≠•È™§ÔºÅ
    * ËßÇÂØüÂà∞ $XX^T A = A$„ÄÇ
        * ÂéüÂõ†ÊòØ $P_X := XX^T$ ÊòØÂà∞ $\text{span}(X)$ ÁöÑÊ≠£‰∫§ÊäïÂΩ±Áü©ÈòµÔºåËÄå $\text{span}(X) = \text{range}(A)$„ÄÇÂõ†Ê≠§ÔºåÂØπ‰∫é $A$ ÁöÑ‰ªª‰ΩïÂàó $a_i$ÔºàÈÉΩÂú® $\text{range}(A)$ ‰∏≠ÔºâÔºå$P_X a_i = a_i$„ÄÇÊâÄ‰ª• $P_X A = A$„ÄÇ
    * Á±ª‰ººÂú∞Ôºå$YY^T A^T = A^T$ÔºàÂõ†‰∏∫ $YY^T$ ÊòØÂà∞ $\text{range}(A^T)$ ÁöÑÊäïÂΩ±ÔºâÔºå‰∏§ËæπÂèñËΩ¨ÁΩÆÂæóÂà∞ $AYY^T = A$„ÄÇ
    * Âõ†Ê≠§Ôºå$A = XX^T A = XX^T (AYY^T) = X(X^T A Y)Y^T$„ÄÇ

* **Êõ¥ÁÆÄÊ¥Å/Á¥ßÂáëÁöÑË°®Ëø∞ÔºàÂêàÂπ∂ÊñπÊ≥ïÔºâÔºö**
    * ‰ª§ $Z = [z_1, \dots, z_k] \in \mathbb{R}^{n \times k}$ Âíå $W = [w_1, \dots, w_k] \in \mathbb{R}^{m \times k}$ ÁöÑÂàóÂêëÈáèÊòØÈöèÊú∫ÁöÑ„ÄÇ
    * ËÆ°ÁÆó $AZ$ Âíå $A^T W$ÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨ÁöÑÂàóÂêëÈáèÂàÜÂà´Ê†áÂáÜÊ≠£‰∫§ÂåñÂæóÂà∞ $X$ Âíå $Y$„ÄÇ ($\text{span}(X) = \text{range}(A) = \text{span}(U)$, $\text{span}(Y) = \text{range}(A^T) = \text{span}(V)$).
    * ÊûÑÈÄ† $\tilde{A} := X^T A Y$ ÔºàÊ≥®ÊÑèÔºöËøôÈÄöÂ∏∏**‰∏çÊòØÂØπÁß∞**Áü©ÈòµÔºÅÔºâÔºåÂπ∂ÂØπÂÖ∂ËøõË°å**ÂÆåÂÖ® SVD** ÂàÜËß£ÂæóÂà∞ $\tilde{A} = \tilde{U} \tilde{\Sigma} \tilde{V}^T$ÔºåÂÖ∂‰∏≠ÊâÄÊúâÂõ†Â≠ê $\tilde{U}, \tilde{\Sigma}, \tilde{V}$ ÈÉΩÊòØ $k \times k$ ÁöÑ„ÄÇ
    * ÁÑ∂ÂêéÊÅ¢Â§ç $U = X \tilde{U}$Ôºå$V = Y \tilde{V}$Ôºå‰ª•Âèä $\Sigma = \tilde{\Sigma}$„ÄÇ


* **‰∏∫‰ªÄ‰πàËøô‰∏™ÂêàÂπ∂ÊñπÊ≥ïËÉΩÂæóÂà∞Áõ∏ÂêåÁöÑÁªìÊûúÔºü**
    * Ê≥®ÊÑè $$\tilde{A} \tilde{A}^T = (X^T A Y)(Y^T A^T X) = X^T A (YY^T) A^T X$$„ÄÇÂõ†‰∏∫ $AYY^T = A$ÔºåÊâÄ‰ª• $$\tilde{A} \tilde{A}^T = X^T A A^T X = B$$„ÄÇÊ†πÊçÆ SVD ÁöÑÊÄßË¥®Ôºå$\tilde{A}$ ÁöÑÂ∑¶Â•áÂºÇÂêëÈáèÔºàÂç≥ $\tilde{U}$ ÁöÑÂàóÔºâÂ∞±ÊòØ $\tilde{A}\tilde{A}^T$ ÁöÑÁâπÂæÅÂêëÈáèÔºå‰πüÂ∞±ÊòØ $B$ ÁöÑÁâπÂæÅÂêëÈáè„ÄÇ
        * ÊâÄ‰ª•Êàë‰ª¨‰∏§ÁßçÊñπÊ≥ïÔºàÂçïÁã¨ R-R ÂíåÂêàÂπ∂ SVDÔºâÂæóÂà∞ÁöÑ $\tilde{U}$ ÊòØ‰∏ÄËá¥ÁöÑÔºåÂõ†Ê≠§ $U = X\tilde{U}$ ÁöÑËÆ°ÁÆóÁªìÊûú‰∏ÄËá¥„ÄÇ
    * ÂêåÊó∂Ôºå$\tilde{A}^T \tilde{A} = (Y^T A^T X)(X^T A Y) = Y^T A^T (XX^T) A Y$„ÄÇÂõ†‰∏∫ $XX^T A = A$ÔºåÊâÄ‰ª• $\tilde{A}^T \tilde{A} = Y^T A^T A Y = C$„ÄÇ$\tilde{A}$ ÁöÑÂè≥Â•áÂºÇÂêëÈáèÔºàÂç≥ $\tilde{V}$ ÁöÑÂàóÔºâÂ∞±ÊòØ $\tilde{A}^T\tilde{A}$ ÁöÑÁâπÂæÅÂêëÈáèÔºå‰πüÂ∞±ÊòØ $C$ ÁöÑÁâπÂæÅÂêëÈáè„ÄÇ
        * ÊâÄ‰ª•Êàë‰ª¨‰∏§ÁßçÊñπÊ≥ïÂæóÂà∞ÁöÑ $\tilde{V}$ ÊòØ‰∏ÄËá¥ÁöÑÔºåÂõ†Ê≠§ $V = Y\tilde{V}$ ÁöÑËÆ°ÁÆóÁªìÊûú‰∏ÄËá¥„ÄÇ
    * Âπ∂‰∏îÔºå$\tilde{A}$ ÁöÑÂ•áÂºÇÂÄº $\tilde{\Sigma}$ ÊòØ $\tilde{A}\tilde{A}^T = B$ Êàñ $\tilde{A}^T\tilde{A} = C$ ÁöÑÁâπÂæÅÂÄºÁöÑÂπ≥ÊñπÊ†πÔºåËøô‰πü‰∏éÂçïÁã¨ÊñπÊ≥ï‰∏ÄËá¥„ÄÇ


## 40.2 Ë∂ÖÂá∫Áß© k ÁöÑÊÉÖÂÜµ

* Êõ¥‰∏ÄËà¨ÁöÑÁÆóÊ≥ïÊòØÁ±ª‰ººÁöÑÔºåÈô§‰∫ÜÊàë‰ª¨ÂÖÅËÆ∏‰ΩøÁî® $\tilde{k} \ge k$ ‰∏™ÈöèÊú∫ÂêëÈáèËøõË°å‰πòÊ≥ïÔºåÂÖ∂‰∏≠ $\tilde{k}$ ÊòØ‰∏Ä‰∏™ÈúÄË¶ÅËÆæÁΩÆÁöÑÂèÇÊï∞ÔºàÁß∞‰∏∫**ËøáÈááÊ†∑ÂèÇÊï∞**, oversampling parameterÔºâ„ÄÇ
    * ÈÄöËøáÈÄâÂèñ $Z \in \mathbb{R}^{n \times \tilde{k}}$ Âíå $W \in \mathbb{R}^{m \times \tilde{k}}$ÔºåÂπ∂‰∏îÂèñË∂≥Â§üÂ§ßÁöÑ $\tilde{k}$ÔºåÊàë‰ª¨‰ªçÂèØ‰ª•ÊúüÊúõ $\text{span}(AZ)$ Âíå $\text{span}(A^T W)$ ÂàÜÂà´**Ëøë‰ººÂú∞ÂåÖÂê´** $A$ ÁöÑÂÄºÂüü $\text{range}(A)$ Âíå $A^T$ ÁöÑÂÄºÂüü $\text{range}(A^T)$Ôºà‰πüÂ∞±ÊòØ $\text{span}(U)$ Âíå $\text{span}(V)$Ôºâ„ÄÇ
    * Âõ†Ê≠§ÔºåÈÄöËøáÂ¶Ç‰∏äÂÆö‰πâ $X = \text{orth}(AZ) \in \mathbb{R}^{m \times \tilde{k}}$ Âíå $Y = \text{orth}(A^T W) \in \mathbb{R}^{n \times \tilde{k}}$ÔºåÊàë‰ª¨‰ªçÁÑ∂ÂèØ‰ª•ÊúüÊúõËøë‰ºº $A \approx XX^T A \approx XX^T A YY^T = X(X^T A Y)Y^T = X \tilde{A} Y^T$ ÊàêÁ´ãÔºåÂÖ∂‰∏≠ $\tilde{A} = X^T A Y \in \mathbb{R}^{\tilde{k} \times \tilde{k}}$ Áé∞Âú®ÊòØ $\tilde{k} \times \tilde{k}$ Áª¥ÁöÑÔºàÂèØËÉΩÊØî $k \times k$ Êõ¥Â§ßÔºâ„ÄÇ
* Ëøô‰∏™Áõ¥ËßâÂæóÂà∞‰∫Ü‰ª•‰∏ãÊäÄÊúØÊÄßÂÆöÁêÜÁöÑÊîØÊíëÔºàËØ•ÂÆöÁêÜË∂ÖÂá∫‰∫ÜÊú¨ËØæÁ®ãËåÉÂõ¥Ôºâ„ÄÇÁâπÂà´Âú∞ÔºåÊàë‰ª¨Áé∞Âú®ÂÅáËÆæ $Z$ÔºàÂíå $W$ÔºâÁöÑÂÖÉÁ¥†ÊòØ**Áã¨Á´ãÂêåÂàÜÂ∏É (i.i.d.) ÁöÑÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏É**ÔºàÈ´òÊñØÂàÜÂ∏ÉÔºâÈöèÊú∫ÂèòÈáè„ÄÇÔºàÂú® Matlab ‰∏≠ÂèØ‰ª•Áî® `randn` ÂáΩÊï∞ÁîüÊàê„ÄÇÔºâ

    **ÂÆöÁêÜ 1**: ËÆæ $\delta > 0$ ‰∏∫Â§±Ë¥•Ê¶ÇÁéá„ÄÇËÆæ $A \in \mathbb{R}^{m \times n}$ÔºåÂπ∂‰ª§ $X = \text{orth}(AZ)$ÔºåÂÖ∂‰∏≠ $Z \in \mathbb{R}^{n \times \tilde{k}}$ ÂÖ∑ÊúâÁã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÊ†áÂáÜÊ≠£ÊÄÅÈ°π„ÄÇÂ≠òÂú®‰∏Ä‰∏™ÈÄöÁî®Â∏∏Êï∞ $C$ ‰ΩøÂæóÂ¶ÇÊûú $\tilde{k} \ge C (k + \log(1/\delta))$ÔºåÈÇ£‰πà‰ª•Ëá≥Â∞ë $1 - \delta$ ÁöÑÊ¶ÇÁéáÔºåÊúâ
     $$||XX^T A||_F \le 2 \sum_{i>k} \sigma_i^2$$ÔºåÂÖ∂‰∏≠ $\sigma_i$ Ë°®Á§∫ $A$ ÁöÑÂ•áÂºÇÂÄº„ÄÇ

* Âõ†Ê≠§ÔºåÂ¶ÇÊûú $A$ ÂÖ∑Êúâ $k$ Èò∂ÁöÑ‚Äú**Êï∞ÂÄºÁß© (numerical rank)**‚ÄùÔºåÂç≥ $\sum_{i>k} \sigma_i^2$ÔºàÂ∞æÈÉ®Â•áÂºÇÂÄºÂπ≥ÊñπÂíåÔºâÂ∞èÂà∞ÂèØ‰ª•ÂøΩÁï•‰∏çËÆ°ÔºåÈÇ£‰πàÈÄöËøáÂèñ $\tilde{k}$ ‰∏∫ÊØî $k$ Á®çÂ§ß‰∏Ä‰∫õÔºà‰æãÂ¶Ç $\tilde{k} = k + p$ÔºåÂÖ∂‰∏≠ $p$ ÊòØÂ∞èÁöÑËøáÈááÊ†∑Êï∞ÔºåÂ¶Ç $p=5$ Êàñ $p=10$ÔºâÔºåÈöèÊú∫ SVD Â∞ÜËÉΩ‰ª•ÂæàÈ´òÁöÑÊ¶ÇÁéá‰øùËØÅÂáÜÁ°ÆÂ∑•‰Ωú„ÄÇ

## 40.3 ÂÆûÈôÖÁÆóÊ≥ï

* **ÁªôÂÆö**:
    * ËÉΩÊâßË°å $A$ Âíå $A^T$ ÁöÑÁü©ÈòµÂêëÈáè‰πòÁßØÁöÑËÉΩÂäõ„ÄÇ
    * ÁõÆÊ†áÁß© $k$„ÄÇ
    * ÈöèÊú∫ÂêëÈáèÁöÑÊï∞Èáè $\tilde{k} \ge k$ÔºàËøáÈááÊ†∑ÂêéÁöÑÁª¥Â∫¶Ôºâ„ÄÇ
* **ÁÆóÊ≥ïÊ≠•È™§**:
    1. ‰ª§ $Z \in \mathbb{R}^{n \times \tilde{k}}$ Âíå $W \in \mathbb{R}^{m \times \tilde{k}}$ ÁöÑÂÖÉÁ¥†‰∏∫Áã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÊ†áÂáÜÊ≠£ÊÄÅÔºàÈ´òÊñØÔºâÈöèÊú∫ÂèòÈáè„ÄÇ
    2. ËÆ°ÁÆó $AZ$ Âíå $A^T W$ÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨ÁöÑÂàóÂêëÈáèÂàÜÂà´**Ê†áÂáÜÊ≠£‰∫§Âåñ (orthonormalize)** ÂæóÂà∞ $X \in \mathbb{R}^{m \times \tilde{k}}$ Âíå $Y \in \mathbb{R}^{n \times \tilde{k}}$„ÄÇ
    3. ÊûÑÈÄ† $\tilde{A} := X^T A Y \in \mathbb{R}^{\tilde{k} \times \tilde{k}}$ ÔºàÊ≥®ÊÑèÔºöÈÄöÂ∏∏‰∏çÂØπÁß∞ÔºÅÔºâÔºåÂπ∂ÂØπÂÖ∂ËøõË°å**ÂÆåÂÖ® SVD** ÂàÜËß£ÂæóÂà∞ $\tilde{A} = \tilde{U} \tilde{\Sigma} \tilde{V}^T$ÔºåÂÖ∂‰∏≠ÊâÄÊúâÂõ†Â≠ê $\tilde{U}, \tilde{\Sigma}, \tilde{V}$ ÈÉΩÊòØ $\tilde{k} \times \tilde{k}$ ÁöÑ„ÄÇ
    4. ÁÑ∂ÂêéÊÅ¢Â§çÂπ∂**Êà™Êñ≠ (truncate)** Ëá≥Áß© $k$Ôºö
        * $\check{U} = [X \tilde{U}]_{:, 1:k} \in \mathbb{R}^{m \times k}$ ÔºàÂèñ $X\tilde{U}$ ÁöÑÂâç $k$ ÂàóÔºâ
        * $\check{V} = [Y \tilde{V}]_{:, 1:k} \in \mathbb{R}^{n \times k}$ ÔºàÂèñ $Y\tilde{V}$ ÁöÑÂâç $k$ ÂàóÔºâ
        * $\check{\Sigma} = [\tilde{\Sigma}]_{1:k, 1:k} \in \mathbb{R}^{k \times k}$ ÔºàÂèñ $\tilde{\Sigma}$ Â∑¶‰∏äËßíÁöÑ $k \times k$ Â≠êÁü©ÈòµÔºâ
* **ËæìÂá∫**:
    * ÈÇ£‰πàÔºå$A \approx \check{U} \check{\Sigma} \check{V}^T$ Â∞±ÊûÑÊàê‰∫Ü $A$ ÁöÑ‰∏Ä‰∏™Ëøë‰ººÁß© $k$ Êà™Êñ≠ SVD„ÄÇ


# 41 Ë∞±ËÅöÁ±ª (Spectral Clustering)

## 41.1 ‰ªéÊï∞ÊçÆÊûÑÈÄ†Âä†ÊùÉÂõæ

* ÂÅáËÆæÊàë‰ª¨ÁªôÂÆö‰∏ÄÁªÑÊï∞ÊçÆÁÇπ $x_1, \dots, x_n \in \mathbb{R}^m$„ÄÇÊàë‰ª¨Â∞ÜÂà©Áî®Ëøô‰∫õÊï∞ÊçÆÁÇπÊù•ÂàõÂª∫‰∏Ä‰∏™**Âä†ÊùÉÂõæ (weighted graph)**„ÄÇ
* ÂêåÊó∂ÁªôÂÆö‰∏Ä‰∏™**Áõ∏‰ººÊÄßÊ†∏ÂáΩÊï∞ (similarity kernel)** $K(x, y) \ge 0$ÔºåÂÆÉÂÆö‰πâ‰∫Ü‰ªªÊÑè‰∏§‰∏™Êï∞ÊçÆÁÇπ $x, y \in \mathbb{R}^m$ ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶„ÄÇËØ•Ê†∏ÂáΩÊï∞Êª°Ë∂≥ÂØπÁß∞ÊÄßÔºöÂØπ‰∫éÊâÄÊúâÁöÑ $x, y$ÔºåÈÉΩÊúâ $K(x, y) = K(y, x)$„ÄÇ
    * ‰∏Ä‰∏™Â∏∏Áî®ÁöÑÈÄâÊã©ÊòØ**È´òÊñØÁõ∏‰ººÊÄßÊ†∏ÂáΩÊï∞ (Gaussian similarity kernel)**Ôºà‰πüÁß∞‰∏∫ÂæÑÂêëÂü∫ÂáΩÊï∞Ê†∏ÔºåRBF kernelÔºâÔºåÂÆÉÊúâ‰∏Ä‰∏™ÂÆΩÂ∫¶ÂèÇÊï∞ $\sigma > 0$Ôºö
        $$ K(x, y) = \exp\left(-\frac{\|x-y\|_2^2}{2\sigma^2}\right) $$
* Âà©Áî®ËØ•Ê†∏ÂáΩÊï∞ÂèØ‰ª•ÊûÑÂª∫ÂõæÁöÑ**Âä†ÊùÉÈÇªÊé•Áü©Èòµ (weighted adjacency matrix)** $A \in \mathbb{R}^{n \times n}$„ÄÇÁü©Èòµ‰∏≠ÁöÑÂÖÉÁ¥† $A_{ij}$ Ë°®Á§∫Á¨¨ $i$ ‰∏™Êï∞ÊçÆÁÇπÔºàÂØπÂ∫î $x_i$ÔºâÂíåÁ¨¨ $j$ ‰∏™Êï∞ÊçÆÁÇπÔºàÂØπÂ∫î $x_j$Ôºâ‰πãÈó¥ÁöÑËæπÁöÑÊùÉÈáçÔºàÂç≥ÂÆÉ‰ª¨ÁöÑÁõ∏‰ººÂ∫¶ÔºâÔºö
    $$ A_{ij} = K(x_i, x_j) $$
    Áî±‰∫éÊ†∏ÂáΩÊï∞ÊòØÂØπÁß∞ÁöÑ ($K(x, y) = K(y, x)$)ÔºåÂõ†Ê≠§ÈÇªÊé•Áü©Èòµ $A$ ‰πüÊòØÂØπÁß∞ÁöÑÔºåÂç≥ $A^T = A$„ÄÇÔºàÊ≥®ÊÑèÔºöÂØπ‰∫éÈ´òÊñØÊ†∏ÔºåÈÄöÂ∏∏ $A_{ii} = K(x_i, x_i) = 1$Ôºå‰ΩÜÊúâÊó∂‰πü‰ºöÂ∞ÜÂØπËßíÁ∫øÂÖÉÁ¥†ËÆæ‰∏∫ 0Ôºâ„ÄÇ

## 41.2 ÂõæÊãâÊôÆÊãâÊñØÁÆóÂ≠ê (Graph Laplacians)

* ÁÑ∂ÂêéÔºåÂèØ‰ª•‰ΩøÁî®Âä†ÊùÉÈÇªÊé•Áü©Èòµ $A$ Êù•ÊûÑÈÄ†Áõ∏Â∫îÁöÑ**ÂõæÊãâÊôÆÊãâÊñØÁü©Èòµ (Graph Laplacian)** $L \in \mathbb{R}^{n \times n}$„ÄÇÂÆÉ‰πüÊòØ‰∏Ä‰∏™ÂØπÁß∞Áü©ÈòµÔºåÂÆö‰πâ‰∏∫ $L = D - A$ÔºåÂÖ∂‰∏≠ $D$ ÊòØ**Â∫¶Áü©Èòµ (degree matrix)**Ôºå‰∏Ä‰∏™ÂØπËßíÁü©ÈòµÔºåÂÖ∂ÂØπËßíÂÖÉÁ¥† $D_{ii} = \sum_{j=1}^n A_{ij}$ÔºàËäÇÁÇπ $i$ ÁöÑÂ∫¶ÔºåÂç≥‰∏éËäÇÁÇπ $i$ Áõ∏ËøûÁöÑÊâÄÊúâËæπÁöÑÊùÉÈáç‰πãÂíåÔºâ„ÄÇÁÆÄÂÜô‰∏∫ $D = \text{diag}(A\mathbf{1}_n)$ÔºàÂÖ∂‰∏≠ $\mathbf{1}_n$ ÊòØÂÖ® 1 ÂêëÈáèÔºâ„ÄÇ
    * Ê≥®ÊÑèÔºå‰ªéÂÖÉÁ¥†‰∏äÁúãÔºå$L_{ij} = \delta_{ij} D_{ii} - A_{ij}$ÔºàÂÖ∂‰∏≠ $\delta_{ij}$ ÊòØÂÖãÁΩóÂÜÖÂÖã Œ¥Ôºâ„ÄÇÂç≥ÂØπËßíÂÖÉÁ¥† $L_{ii} = D_{ii} - A_{ii}$ÔºåÈùûÂØπËßíÂÖÉÁ¥† $L_{ij} = -A_{ij}$ ($i \neq j$)„ÄÇ

* **Ë°•ÂÖÖËØ¥Êòé**ÔºöÂõæÊãâÊôÆÊãâÊñØÁÆóÂ≠êÁöÑ‰∏ÄÁßçÁõ¥ËßÇÁêÜËß£Êù•Ëá™‰∫éËÄÉËôë**Â∑¶ÂΩí‰∏ÄÂåñÂõæÊãâÊôÆÊãâÊñØÁÆóÂ≠ê (left normalized graph Laplacian)** $\tilde{L} = D^{-1} L = I - D^{-1} A$ÔºåÊ≥®ÊÑèÂÆÉÈÄöÂ∏∏ÊòØ**‰∏çÂØπÁß∞ÁöÑ**ÔºÅ
    * ‰ªéÂÖÉÁ¥†‰∏äÁúãÔºå$\tilde{L}_{ij} = \delta_{ij} - A_{ij} / D_{ii}$„ÄÇ
    * ÈÇ£‰πàÂØπ‰∫é‰ªªÊÑèÂêëÈáè $v \in \mathbb{R}^n$Ôºå‰πòÁßØÂêëÈáè $[\tilde{L}v]$ ÁöÑÁ¨¨ $i$ ‰∏™ÂÖÉÁ¥†ÊòØÔºö
        $$ [\tilde{L}v]_i = v_i - \frac{\sum_{j=1}^n A_{ij} v_j}{\sum_{j=1}^n A_{ij}} $$
        (ËøôÈáåÂÅáËÆæ $D_{ii} = \sum_j A_{ij} \neq 0$)
    * Âõ†Ê≠§ÔºåÂ¶ÇÊûúÊàë‰ª¨Â∞ÜÂêëÈáè $v$ Áúã‰ΩúÂõæ‰∏äÁöÑ‰∏Ä‰∏™‚ÄúÂáΩÊï∞‚ÄùÔºàÂ∞ÜËäÇÁÇπ $i$ Êò†Â∞ÑÂà∞ÂÄº $v_i$ÔºâÔºåÈÇ£‰πà $\tilde{L}v$ Â∞±ÂØπÂ∫îËøôÊ†∑‰∏Ä‰∏™ÂáΩÊï∞ÔºöÂÆÉÊµãÈáè‰∫ÜËäÇÁÇπ $i$ ‰∏äÁöÑÂáΩÊï∞ÂÄº $v_i$ ‰∏éÂÖ∂**ÈÇªÂ±ÖËäÇÁÇπÂÄºÁöÑÂä†ÊùÉÂπ≥ÂùáÂÄº**‰πãÈó¥ÁöÑ**Â∑ÆÂÄº**„ÄÇ

* ÂÖ≥‰∫é $L$ ÁöÑ‰∏§‰∏™ÊÄßË¥®ÔºàËØæÂ†Ç‰∏äÁöÑËØÅÊòéÂú®Ê≠§Áï•ÂéªÔºå‰Ωú‰∏∫ÁªÉ‰π†ÔºâÔºö
    * **ÊÄßË¥® 1**: $L\mathbf{1}_n = 0$„ÄÇËøôÊÑèÂë≥ÁùÄ 0 ÊòØ $L$ ÁöÑ‰∏Ä‰∏™ÁâπÂæÅÂÄºÔºåÂÖ® 1 ÂêëÈáè $\mathbf{1}_n$ ÊòØÂØπÂ∫îÁöÑÁâπÂæÅÂêëÈáè„ÄÇ
    * **ÊÄßË¥® 2**: $L$ ÊòØÔºàÂº±Ôºâ**ÂØπËßíÂç†‰ºò (diagonally dominant)** ÁöÑÔºåÂç≥ÂØπ‰∫éÊâÄÊúâ $i=1, \dots, n$ÔºåÊúâ $|L_{ii}| \ge \sum_{j \neq i} |L_{ij}|$„ÄÇÔºàÂ¶ÇÊûú $A_{ij} \ge 0$ ‰∏î $A_{ii}=0$, Âàô $L_{ii} = \sum_{j \neq i} A_{ij} = \sum_{j \neq i} |L_{ij}|$ÔºåÂç≥Ë°åÂíå‰∏∫Èõ∂Ôºâ„ÄÇ

>[!proof]
>Let's now prove the propertis: By definition, we have 
> $$
(L1_{n})_{i} = \sum_{j=1}^{n} L_{ij}(1_{n})_{j} = \sum_{j=1}^{n} L_{ij}= L_{ii} + \sum_{j\neq i} L_{ij}
>$$
>By definition, we have $L_{ii} = D_{ii} -A_{ii}$ and $L_{ij} = -A_{ij}$, so $(L1_{n})_{i}=D_{ii} - \sum_{j=1}^{n}A_{ij}=D_{ii} -D_{ii} =0$
>Then let's prove $L$ is weakly diagonally dominant: still by definition, we have $\sum_{j\neq i}\lvert L_{ij} \rvert=\sum_{j\neq i}\lvert -A_{ij} \rvert=\sum_{j\neq i}\lvert A_{ij} \rvert$
>For the LHS, we know $\lvert L_{ii} \rvert= \lvert D_{ii} - A_{ii} \rvert=\left\lvert  \sum A_{ij} - A_{ii}  \right\rvert=\underbrace{ \left\lvert  \sum_{j\neq i}A_{ij}  \right\rvert=\sum_{j\neq i}\lvert A_{ij} \rvert }_{ \text{given }A_{ij}\geq 0 }$

* Áî±‰∫é $L$ ÊòØÂØπËßíÂç†‰ºò‰∏îÂØπËßíÂÖÉÁ¥†ÈùûË¥üÔºàÂõ†‰∏∫ $D_{ii} = \sum_j A_{ij} \ge A_{ii}$ Â¶ÇÊûú $A_{ij} \ge 0$ÔºâÔºåÂèØ‰ª•Êé®ÂØºÂá∫Á¨¨‰∏â‰∏™ÊÄßË¥®ÔºàËØÅÊòéÁï•ÂéªÔºâ‚Äî‚Äî
    * **ÊÄßË¥® 3**: $L$ ÊòØ**ÂçäÊ≠£ÂÆö (positive semidefinite)** ÁöÑÔºàÂç≥ $L$ ÁöÑÊâÄÊúâÁâπÂæÅÂÄºÈÉΩ $\ge 0$Ôºâ„ÄÇ
* ÂÆûÈôÖ‰∏äÔºåË∞±ÂµåÂÖ•/ËÅöÁ±ªÈÄöÂ∏∏‰ΩøÁî®**ÂØπÁß∞ÂΩí‰∏ÄÂåñÂõæÊãâÊôÆÊãâÊñØÁÆóÂ≠ê (symmetric normalized graph Laplacian)** $$\hat{L} := D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2}$$„ÄÇÂÖ∂ÂÖÉÁ¥†‰∏∫ $$\hat{L}_{ij} = \delta_{ij} - \frac{A_{ij}}{\sqrt{D_{ii} D_{jj}}}$$„ÄÇ

>[!Degree Matrix]
>The matrix $D$ here is called the degree matrix, where $D=\text{diag}(\left\{ D_{ii} \right\}_{i=1}^{n})$ such that $D_{ii}=\sum_{j=1}^{n}A_{ij}$. We call $D_{ii}$ to be the degree of each node $i$

* Êàë‰ª¨‰ª•ÊúÄÂêé‰∏ÄÊù°ÊÄßË¥®ÁªìÊùüËÆ®ËÆ∫Ôºö
* **ÊÄßË¥® 4**: $\hat{L}$ (*Âíå* $\tilde{L}$)ÁöÑÊâÄÊúâÁâπÂæÅÂÄºÈÉΩ‰Ωç‰∫éÂå∫Èó¥ $[0, 2]$ ÂÜÖ„ÄÇ
    * **ËØÅÊòéÊ¶ÇË¶Å** [ËØæÂ†ÇËÆ≤Êéà‰∏≠Áï•Ëøá]Ôºö
        * È¶ñÂÖàÔºå$\hat{L}$ ÊòØÂçäÊ≠£ÂÆöÁöÑ„ÄÇËøôÊòØÂõ†‰∏∫ÂÆÉÂèØ‰ª•ÈÄöËøá $\hat{L} = (D^{-1/2}) L (D^{-1/2})^T$ ÊûÑÂª∫ÔºàÂõ†‰∏∫ $D^{-1/2}$ ÂØπÁß∞ÔºâÔºåÂπ∂‰∏î $L$ ÊòØÂçäÊ≠£ÂÆöÁöÑÔºàÊÄßË¥® 3Ôºâ„ÄÇÂõ†Ê≠§ $\hat{L}$ ÁöÑÁâπÂæÅÂÄºÈÉΩ $\ge 0$„ÄÇ
        > by simply verify $y'\hat{L}y\geq 0$ for arbitrary $y \in \mathbb{R}^{n}$
        * Ê≥®ÊÑèÂà∞ $\hat{L}$ Âíå $\tilde{L}$ ÊòØ**Áõ∏‰ºº (similar)** ÁöÑÔºåÂÆÉ‰ª¨‰πãÈó¥ÈÄöËøá $\tilde{L} = D^{1/2} \hat{L} D^{-1/2}$ Áõ∏ÂÖ≥ËÅîÔºàÊàñËÄÖ $\hat{L} = D^{-1/2} \tilde{L} D^{1/2}$Ôºâ„ÄÇÁõ∏‰ººÁü©ÈòµÊã•ÊúâÁõ∏ÂêåÁöÑÁâπÂæÅÂÄº„ÄÇ
        * ÊâÄ‰ª•ÔºåÂè™ÈúÄË¶ÅËØÅÊòé $\tilde{L}$ ÁöÑË∞±ÂçäÂæÑ $\rho(\tilde{L}) \le 2$ Âç≥ÂèØ„ÄÇ
        * Ê†πÊçÆ Gerschgorin ÂúÜÁõòÂÆöÁêÜÊàñËåÉÊï∞ÁêÜËÆ∫ÔºåËØÅÊòé $\|\tilde{L}\|_\infty \le 2$ ÔºàÊúÄÂ§ßË°åÁªùÂØπÂÄºÂíåËåÉÊï∞ÔºâÂ∞±Ë∂≥Â§ü‰∫Ü„ÄÇ
        >for any induced matrix norm, we have $\rho(M)\leq \lVert M \rVert$
        * Êàë‰ª¨ÂèØ‰ª•ÁïåÂÆö $\tilde{L}$ ÁöÑ‰ªªÊÑè‰∏ÄË°åÁöÑÁªùÂØπÂÄº‰πãÂíåÔºàÂÅáËÆæ $A_{ij} \ge 0$ÔºåËøôÊòØÁõ∏‰ººÊÄßÊ†∏ÂáΩÊï∞ÁöÑÈÄöÂ∏∏ÊÉÖÂÜµÔºâÔºö
            $$ \sum_{j=1}^n |\tilde{L}_{ij}| = \sum_{j=1}^n |\delta_{ij} - \frac{A_{ij}}{D_{ii}}| \le \sum_{j=1}^n |\delta_{ij}| + \sum_{j=1}^n |\frac{-A_{ij}}{D_{ii}}| = 1 + \frac{1}{D_{ii}} \sum_{j=1}^n A_{ij} = 1 + \frac{D_{ii}}{D_{ii}} = 2 $$
            *(Ëøô‰∏™ÁïåÈôê‰ΩøÁî®‰∫Ü‰∏âËßí‰∏çÁ≠âÂºèÂíå $A_{ij} \ge 0$)*„ÄÇ
        * Âõ†Ê≠§Ôºå$\tilde{L}$ ÁöÑÊâÄÊúâË°åÁöÑÁªùÂØπÂÄºÂíåÈÉΩ‰ª• 2 ‰∏∫ÁïåÔºåËøôÊÑèÂë≥ÁùÄ $\|\tilde{L}\|_\infty \le 2$Ôºå‰ªéËÄå $\rho(\tilde{L}) \le 2$„ÄÇ
        * ÁªìÂêàÁâπÂæÅÂÄºÈùûË¥üÔºåÂèØÁü• $\hat{L}$ (Âíå $\tilde{L}$) ÁöÑÁâπÂæÅÂÄºÈÉΩÂú® $[0, 2]$ Âå∫Èó¥ÂÜÖ„ÄÇËØÅÊØï„ÄÇ
