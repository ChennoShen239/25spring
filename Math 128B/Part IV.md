# 非线性方程组与优化 (Nonlinear Equations and Optimization)

## 42 一般非线性方程组

* 一个包含 $n$ 个未知数、$n$ 个方程的**一般非线性方程组 (general system of nonlinear equations)** 问题可以表述为求解 $F(x) = 0$，其中 $F: \mathbb{R}^n \to \mathbb{R}^n$ 是一个给定的函数（从 $n$ 维实向量空间映射到自身的函数）。
    * 一般来说，这样的方程组可能存在任意数量的解（包括零个解、一个解或多个解）。
    * 我们将重点关注**迭代方法 (iterative methods)**，用于寻找（假定存在的）某个解，通常是寻找离某个**初始猜测 (initial guess)** “最近”的那个解。
    * 将 $F(x)$ 视为一个向量会很有帮助：
        $$ F(x) = \begin{pmatrix} f_1(x) \\ \vdots \\ f_n(x) \end{pmatrix} $$
        其中的每个分量 $f_i: \mathbb{R}^n \to \mathbb{R}$ 都是一个标量值函数 (scalar-valued function)。
* 例子包括：
    * **线性方程组 (Linear equations)**：这是当 $F(x) = Ax - b$ 时的特殊情况。
        * 我们已经相当详细地讨论过线性方程组了！
        * 稍后（在牛顿法部分）我们将看到求解线性方程组如何帮助求解一般的非线性方程组。
    * **函数优化 (Optimization)**：对于一个函数 $u: \mathbb{R}^n \to \mathbb{R}$，寻找其**临界点 (critical points)**（可能对应于最小值、最大值或鞍点）需要求解其梯度为零的方程 $\nabla u(x) = 0$。在这种情况下，我们可以视 $F(x) = \nabla u(x)$。
        * 我们下面会几次回到这个设定。
        * 特别地，当函数 $u$ 是**严格凸函数 (strictly convex function)** 时，可以获得最好的（收敛性）保证。
        * 这意味着其**海森矩阵 (Hessian matrix)** $$\nabla^2 u(x) = \left[ \frac{\partial^2 u}{\partial x_i \partial x_j}(x) \right]$$ 对于所有的 $x$ 都是**正定矩阵 (positive definite matrix)**。（我们稍后会回到这一点。）
    * **其他来源**: 非线性方程组出现在无数场景中，例如求解非线性常微分方程 (ODE) 的隐式时间步进方法、非线性偏微分方程 (PDE) 的离散化等等……

# 43 不动点迭代 (Fixed-Point Iteration)

## 43.1 基本思想

* **定义**: 给定一个函数 $G: \mathbb{R}^n \to \mathbb{R}^n$，如果 $G(x) = x$ 成立，我们称 $x$ 是 $G$ 的一个**不动点 (fixed point)**。
* 不动点迭代的想法始于：找到一个函数 $G$，使得 $G$ 的不动点**恰好**是原方程 $F(x) = 0$ 的解。
    * 构造这样一个 $G$ 的一种方法是，对于任意给定的 $\alpha \neq 0$，取
        $$ G(x) := x - \alpha F(x) $$
    * 那么注意到 $G(x) = x \Leftrightarrow x - \alpha F(x) = x \Leftrightarrow -\alpha F(x) = 0 \Leftrightarrow F(x) = 0$（因为 $\alpha \neq 0$）。这表明 $G$ 的不动点确实就是 $F(x)=0$ 的解，符合要求。
    * 尽管任何非零 $\alpha$ 的选择都能在上述意义下“奏效”（即保证不动点等价于原方程解），但不同的 $\alpha$ 选择可能导致下面介绍的不动点迭代收敛或发散。
* 给定这样的函数 $G$，**不动点迭代 (fixed-point iteration)** 指的是通过递归定义产生的序列 $x^{(k)}$：
    $$ x^{(k+1)} = G(x^{(k)}), \quad k = 0, 1, 2, \dots $$
    其中 $x^{(0)} \in \mathbb{R}^n$ 是某个初始猜测。换句话说，$x^{(k)} = \underbrace{G(G(\dots G(}_{k \text{ 次}}x^{(0)}) \dots))$ （将 $G$ 作用 $k$ 次）。
    
* 为了继续分析，我们需要假设函数 $G$ 是**连续的 (continuous)**。为完整起见，我们给出多变量情况下的定义。
    * **定义**: 我们称函数 $G: \mathbb{R}^n \to \mathbb{R}^n$ 是连续的，如果对于任意 $x \in \mathbb{R}^n$ 以及任意满足 $\lim_{k\to\infty} x^{(k)} = x$ 的序列 $\{x^{(k)}\}$，都有 $\lim_{k\to\infty} G(x^{(k)}) = G(x)$ 成立。

* **想法**: 我们希望不动点迭代是**收敛的 (converges)**。
* **断言**: 如果迭代确实收敛，并且 $G$ 是连续的，那么它的极限点必然是 $G$ 的一个不动点。
    * **断言证明**: 设 $x = \lim_{k\to\infty} x^{(k)}$（我们假设这个极限存在）。对迭代关系式 $x^{(k+1)} = G(x^{(k)})$ 两边同时取 $k \to \infty$ 的极限。
        * 左边 $\lim_{k\to\infty} x^{(k+1)} = x$ （因为如果序列收敛，其子序列也收敛到同一极限）。
        * 右边 $\lim_{k\to\infty} G(x^{(k)})$。由于 $G$ 是连续的且 $x^{(k)} \to x$，根据连续性定义，$\lim_{k\to\infty} G(x^{(k)}) = G(\lim_{k\to\infty} x^{(k)}) = G(x)$。
        因此，我们得到 $x = G(x)$。这恰好说明极限点 $x$ 是 $G$ 的一个不动点。证毕。

## 43.2 巴拿赫不动点定理 (Banach Fixed-Point Theorem)

* 不动点迭代何时收敛？
* **巴拿赫不动点定理**提供了一个**充分条件**。为了证明它，我们需要做一个技术性定义并回顾数学分析中的一个要点。在下面的讨论中，我们选定 $\mathbb{R}^n$ 上的某种范数 $\|\cdot\|$，并用 $\|\cdot\|$ 表示 $\mathbb{R}^{n \times n}$ 矩阵上的相应诱导范数（或称算子范数）。
    * **定义 (柯西序列, Cauchy Sequence)**：我们称序列 $\{x^{(k)}\}$ 是**柯西序列**，如果对于任意给定的 $\epsilon > 0$，存在一个整数 $K$，使得对于所有 $k, l \ge K$，都有 $\|x^{(k)} - x^{(l)}\| \le \epsilon$ 成立。
    * **事实**: （在 $\mathbb{R}^n$ 中）任意柯西序列都收敛到某个极限点。
        * 更一般地，这个结论在**完备度量空间 (complete metric spaces)** 中对柯西序列都成立。（$\mathbb{R}^n$ 配备任何范数都是一个完备度量空间。）
* **定义 (压缩映射, Contraction Map)**：我们称映射 $G: \mathbb{R}^n \to \mathbb{R}^n$ 是一个**压缩映射**（带有参数 $\alpha \in [0, 1)$），如果对于所有 $x, y \in \mathbb{R}^n$，都满足：
    $$ \|G(x) - G(y)\| \le \alpha \|x - y\| $$
    其中 $\alpha$ 是一个小于 1 的非负常数（称为压缩因子）。
> This is just a Lipschitz condition with $L = \alpha<1$.

**定理 (巴拿赫不动点定理)**. 假设 $G: \mathbb{R}^n \to \mathbb{R}^n$ 是一个**压缩映射**（参数为 $\alpha \in [0, 1)$）。令 $x^{(0)} \in \mathbb{R}^n$ 并通过 $x^{(k+1)} = G(x^{(k)})$ ($k=0, 1, 2, \dots$) 递归定义序列 $\{x^{(k)}\}$。那么：
1.  该序列 $\{x^{(k)}\}$ **收敛**到一个极限点 $x^* \in \mathbb{R}^n$，即 $\lim_{k\to\infty} x^{(k)} = x^*$。
2.  极限点 $x^*$ 是 $G$ 在 $\mathbb{R}^n$ 中的**唯一**不动点。
3.  **误差界 (Error Bound)** 满足：
    $$ \|x^{(k)} - x^*\| \le \frac{\alpha^k}{1 - \alpha} \|x^{(1)} - x^{(0)}\| $$
---
* 该定理不仅保证了当 $G$ 是压缩映射时不动点迭代**必定收敛**到唯一的不动点，而且还提供了一个**收敛速度**的估计。
    * 误差 $\|x^{(k)} - x^*\|$ 随着迭代次数 $k$ 的增加**指数级衰减 (exponentially decays)**，衰减因子为 $\alpha \in [0, 1)$。
    * 在数值分析中，我们实际上称这种收敛为**线性收敛 (linear convergence)**，因为误差的对数 $\log \|x^{(k)} - x^*\|$ 近似地以 $\log \alpha < 0$ 的斜率线性下降。
    * 线性收敛的一种解释是，获得**每一位新的精度数字**（例如，误差减少 10 倍）大致需要**相同数量的迭代次数**。
    * 我们稍后将看到，牛顿法 (Newton's method) 可以达到**超线性收敛 (superlinear convergence)**（通常是二次收敛），在这种情况下，获得每一位新的精度数字所需的迭代次数会（渐进地）越来越少。

* **定理证明概要**:
    * **第一步：证明序列 $\{x^{(k)}\}$ 是柯西序列。**
        * 对于 $k \le l$，利用伸缩和 (telescoping sum) 的思想和三角不等式 (triangle inequality)，我们可以得到界限：
            $$ \|x^{(k)} - x^{(l)}\| = \| \sum_{i=k}^{l-1} (x^{(i)} - x^{(i+1)}) \| \le \sum_{i=k}^{l-1} \|x^{(i)} - x^{(i+1)}\| $$
        * **子断言**: $G^j$ ($G$ 的 $j$ 次复合) 是一个压缩映射，其参数为 $\alpha^j$。
        * 可通过归纳法证明：$$\begin{align}
\|G^j(x) - G^j(y)\|  & = \|G(G^{j-1}(x)) - G(G^{j-1}(y))\|  \\
 & \le \alpha \|G^{j-1}(x) - G^{j-1}(y)\|  \\
 & \le \dots \le \alpha^j \|x-y\|
\end{align}$$
        * 利用子断言和 $x^{(i+1)}=G(x^{(i)})$：
            $$ \|x^{(i)} - x^{(i+1)}\| = \|G(x^{(i-1)}) - G(x^{(i)})\| = \|G^i(x^{(0)}) - G^i(x^{(1)})\| $$
            （注意 $x^{(1)}=G(x^{(0)})$）。应用 $G^i$ 的压缩性质：
            $$ \|x^{(i)} - x^{(i+1)}\| \le \alpha^i \|x^{(0)} - x^{(1)}\| $$
        * 将此代入之前的求和界限：
            $$ \|x^{(k)} - x^{(l)}\| \le \sum_{i=k}^{l-1} \alpha^i \|x^{(0)} - x^{(1)}\| = \left( \sum_{i=k}^{l-1} \alpha^i \right) \|x^{(0)} - x^{(1)}\| $$
        * 因为 $\alpha \in [0, 1)$，这个有限几何级数的和可以被无限几何级数的和所界定：
            $$ \sum_{i=k}^{l-1} \alpha^i \le \sum_{i=k}^{\infty} \alpha^i = \alpha^k + \alpha^{k+1} + \dots = \frac{\alpha^k}{1 - \alpha} $$
        * 因此，对于任意 $l \ge k$，我们有：
            $$ \|x^{(k)} - x^{(l)}\| \le \frac{\alpha^k}{1 - \alpha} \|x^{(0)} - x^{(1)}\| \quad (*) $$
>[!Note]
>It's important to see that the RHS of $(*)$ has nothing to do with $l$.
        
* 由于 $\alpha \in [0, 1)$，当 $k \to \infty$ 时，右边的界限 $\frac{\alpha^k}{1 - \alpha} \|x^{(0)} - x^{(1)}\| \to 0$。这意味着*对于任意 $\epsilon > 0$，总能找到足够大的 $K$ 使得当 $k, l \ge K$ 时，$\|x^{(k)} - x^{(l)}\|$ 小于 $\epsilon$*。所以，该序列 $\{x^{(k)}\}$ 是柯西序列。
    * **第二步：证明收敛性和误差界。**
        * 因为 $\{x^{(k)}\}$ 是 $\mathbb{R}^n$（完备空间）中的柯西序列，它必定收敛到一个极限点 $x^* = \lim_{k\to\infty} x^{(k)}$。
        * 因为压缩映射必然是连续的，根据 43.1 的证明，极限点 $x^*$ 必定是 $G$ 的一个不动点。
        * 不动点的唯一性可以通过反证法证明：
	        * 若 $x^*$ 和 $y^*$ 都是不动点且 $x^* \neq y^*$，则 $\|x^*-y^*\| = \|G(x^*)-G(y^*)\| \le \alpha \|x^*-y^*\|$。
	        * 因为 $\alpha < 1$，这只有在 $\|x^*-y^*\|=0$ 即 $x^*=y^*$ 时才可能成立，与假设矛盾。
        * 现在，在不等式 $(*)$ 中保持 $k$ 固定，令 $l \to \infty$。由于范数 $\|\cdot\|$ 是连续函数，我们可以将极限移到范数内部：
            $$ \lim_{l\to\infty} \|x^{(k)} - x^{(l)}\| = \|x^{(k)} - \lim_{l\to\infty} x^{(l)}\| = \|x^{(k)} - x^*\| $$
        * 因此，从 $(*)$ 直接得到误差界：
            $$ \|x^{(k)} - x^*\| \le \frac{\alpha^k}{1 - \alpha} \|x^{(0)} - x^{(1)}\| $$
    * 定理证毕。

## 43.3 压缩映射的充分条件

* 对于一个映射是否是压缩映射，有一个很有用的**充分条件 (sufficient condition)**，它是根据**一阶偏导数 (first partial derivatives)** 来表述的。
* 为了陈述它，我们需要定义向量值函数的**雅可比矩阵 (Jacobian matrix)**。
* **定义 (雅可比矩阵)**：对于一个函数 $G: \mathbb{R}^n \to \mathbb{R}^n$，其分量函数为 $g_1, \dots, g_n$（即 $G(x) = (g_1(x), \dots, g_n(x))^T$），其**雅可比矩阵** $DG(x)$（有时也记作 $J_G(x)$ 或 $\nabla G(x)$）是一个 $n \times n$ 的矩阵值函数，定义为其元素为：
    $$ [DG(x)]_{ij} = \frac{\partial g_i}{\partial x_j}(x) $$
    （即第 $i$ 行是第 $i$ 个分量函数 $g_i$ 的梯度转置，或者第 $j$ 列是 $G$ 对 $x_j$ 的偏导数向量）。

**定理**. 假设存在一个常数 $\alpha \in [0, 1)$，使得对于定义域中（此处为 $\mathbb{R}^n$）的所有 $x$，雅可比矩阵 $DG(x)$ 的**矩阵范数**满足 $\|DG(x)\| \le \alpha$。那么，$G$ 就是一个压缩映射，其压缩因子（参数）为 $\alpha$。

（注意：这里的矩阵范数 $\|\cdot\|$ 是与定义压缩映射时所使用的向量范数 $\|\cdot\|$ 相对应的诱导范数，例如，如果向量范数是 2-范数，则矩阵范数是谱范数）。

* **证明**:
    * 令 $x, y \in \mathbb{R}^n$ 是定义域中任意两个不同的点。我们想要证明 $\|G(x) - G(y)\| \le \alpha \|x - y\|$。
    * 我们将使用**微积分基本定理 (Fundamental Theorem of Calculus)** 的向量形式。定义连接 $x$ 和 $y$ 的直线段参数曲线 $$z(t) = (1 - t)x + ty$$，其中 $t \in [0, 1]$。因此 $z(0) = x$，$z(1) = y$，并且 $$z'(t) = \frac{dz}{dt} = -x + y = y - x$$
    * 根据微积分基本定理：
        $$ G(y) - G(x) = G(z(1)) - G(z(0)) = \int_0^1 \frac{d}{dt} G(z(t)) dt $$
    * 应用向量函数的链式法则 (Chain Rule)，$\frac{d}{dt} G(z(t)) = DG(z(t)) z'(t)$。所以：
        $$ G(y) - G(x) = \int_0^1 DG(z(t)) z'(t) dt = \int_0^1 DG(z(t)) [y - x] dt $$
    * 对上式两边取范数，并应用积分形式的三角不等式（即范数的积分小于等于积分的范数：$\|\int f(t) dt\| \le \int \|f(t)\| dt$）：
        $$ \|G(y) - G(x)\| = \left\| \int_0^1 DG(z(t)) [y - x] dt \right\| \le \int_0^1 \|DG(z(t)) [y - x]\| dt $$
    * 利用诱导矩阵范数的性质 $\|Mv\| \le \|M\| \|v\|$：
        $$ \|G(y) - G(x)\| \le \int_0^1 \|DG(z(t))\| \|y - x\| dt $$
    * 根据定理的假设，对于路径 $z(t)$ 上的所有点（因为 $t \in [0,1]$ 时 $z(t)$ 位于连接 $x,y$ 的线段上，属于 $\mathbb{R}^n$），都有 $\|DG(z(t))\| \le \alpha$：
        $$ \|G(y) - G(x)\| \le \int_0^1 \alpha \|y - x\| dt $$
    * 由于 $\alpha \|y - x\|$ 相对于积分变量 $t$ 是常数：
        $$ \|G(y) - G(x)\| \le \alpha \|y - x\| \int_0^1 dt = \alpha \|y - x\| \cdot 1 = \alpha \|y - x\| $$
    * 因为这个不等式对任意 $x, y \in \mathbb{R}^n$ 都成立，并且 $\alpha \in [0, 1)$，所以根据定义，$G$ 是一个压缩映射，其压缩因子为 $\alpha$。证明完毕。

## 43.4 梯度下降法在凸优化中的收敛性

* 考虑函数 $u: \mathbb{R}^n \to \mathbb{R}$ 的优化问题（通常是最小化问题）。寻找其临界点需要求解 $F(x) = \nabla u(x) = 0$。我们可以采用不动点迭代 $$G(x) = x - \epsilon \nabla u(x)$$ 来求解，其中 $\epsilon$ 是**步长 (step size)** 参数，其作用我们将分析。
* **定义 (严格凸函数)**：我们称函数 $u \in C^2(\mathbb{R}^n)$（意即 $u$ 具有连续的二阶偏导数）是**严格凸 (strictly convex)** 的，如果其**海森矩阵 (Hessian matrix)** $$\nabla^2 u(x) := \left[ \frac{\partial^2 u}{\partial x_i \partial x_j}(x) \right]$$ 对所有 $x$ 都是**正定 (positive definite)** 的。
    * $C^2$ 假设保证了海森矩阵存在且对称（因为此时混合偏导数可交换顺序）。
* 我们将特别假设 $u$ 是严格凸的，这等价于说 $\nabla^2 u(x)$ 的所有特征值都大于 0。
    * 严格凸函数的任何临界点 $x$（满足 $\nabla u(x) = 0$）必定是**唯一的全局最优解 (global optimizer)**（通常是最小值点）。
    * 我们将做一个更具体的假设，即强凸性 (strong convexity) 和梯度 Lipschitz 连续性。

* **假设 (强凸性与梯度 Lipschitz)**：假设存在常数 $c, C > 0$，使得对于所有 $x$，海森矩阵 $\nabla^2 u(x)$ 的所有特征值 $\lambda_i(\nabla^2 u(x))$ 都位于区间 $[c, C]$ 内。
    * （注：$\lambda_i \ge c > 0$ 保证了强凸性；$\lambda_i \le C$ 保证了梯度 $\nabla u$ 是 Lipschitz 连续的，Lipschitz 常数为 $C$）。
    * 正如我们将看到的，比率 $C/c$ 非常关键，可以通俗地称之为该优化问题的**条件数 (condition number)**，这源于正定矩阵的条件数（在 2-范数下）是其最大与最小特征值之比。
    
* 对于 $G(x) = x - \epsilon \nabla u(x)$ 的不动点迭代具体给出了更新规则：$$x^{(k+1)} = x^{(k)} - \epsilon \nabla u(x^{(k)})$$。这被称为**梯度下降法 (gradient descent)**，步长为 $\epsilon > 0$。

**定理**. 对于满足上述假设（$\nabla^2 u(x)$ 特征值在 $[c, C]$ 内， $0 < c \le C$）的函数 $u$，采用步长 $\epsilon = 1/C$ 的梯度下降法 $$x^{(k+1)} = x^{(k)} - \frac{1}{C} \nabla u(x^{(k)})$$ 收敛到 $u$ 的唯一全局最优解 $x^*$。此外，该梯度下降的映射 $$G(x) = x - \frac{1}{C} \nabla u(x)$$ 是一个压缩映射，其压缩因子为 $$\alpha = 1 - \frac{c}{C}$$。因此根据巴拿赫不动点定理，我们有（某种形式的）收敛界
 $$\|x^{(k)} - x^*\|_2 \le (1 - c/C)^k (C/c) \|x^{(1)} - x^{(0)}\|_2$$

* **证明**:
    * **目标**: 证明 $G(x) = x - \frac{1}{C}\nabla u(x)$ 是一个压缩映射，压缩因子为 $\alpha = 1-c/C$。根据上一节的定理，只需证明其雅可比矩阵 $DG(x)$ 的范数 $\|DG(x)\|_2 \le 1-c/C$。
    * **计算雅可比矩阵**:
		$$ DG(x) = \nabla G(x) = \nabla (x - \epsilon \nabla u(x)) = \nabla x - \epsilon \nabla(\nabla u(x)) = I_n - \epsilon \nabla^2 u(x) $$
        其中 $I_n$ 是 $n$ 阶单位矩阵。
    * **分析特征值**: 当步长 $\epsilon = 1/C$ 时：
        $$ DG(x) = I_n - \frac{1}{C} \nabla^2 u(x) $$
        设 $\lambda_i(\nabla^2 u)$ 是 $\nabla^2 u(x)$ 的特征值，根据假设我们知道 $c \le \lambda_i(\nabla^2 u) \le C$。
        那么 $DG(x)$ 的特征值 $\lambda_i(DG)$ 满足：
        $$ \lambda_i(DG) = 1 - \frac{1}{C} \lambda_i(\nabla^2 u) $$
        因为 $c \le \lambda_i(\nabla^2 u) \le C$，所以 $\frac{c}{C} \le \frac{1}{C} \lambda_i(\nabla^2 u) \le \frac{C}{C} = 1$。
        因此，$1 - 1 \le 1 - \frac{1}{C} \lambda_i(\nabla^2 u) \le 1 - \frac{c}{C}$，即 $DG(x)$ 的所有特征值都位于区间 $[0, 1 - c/C]$ 内。
    * **计算范数**: 由于 $\nabla^2 u(x)$ 是对称的，$DG(x) = I - \frac{1}{C} \nabla^2 u(x)$ 也是对称矩阵。对于对称矩阵，其 2-范数（谱范数）等于其谱半径（特征值绝对值的最大值）：$$\|DG(x)\|_2 = \rho(DG(x)) = \max_i |\lambda_i(DG)|$$
        因为 $DG(x)$ 的特征值在 $[0, 1 - c/C]$ 内，并且 $c, C > 0$ 且 $c \le C$ 保证了 $0 \le 1 - c/C \le 1$，所以 $\rho(DG(x)) = \max_i |\lambda_i(DG)| = 1 - c/C$。
        因此，$\|DG(x)\|_2 = 1 - c/C$。
> [!2-norm]
> Definition is $\lVert A \rVert_{2}= \max_{\lVert x \rVert=1}\lVert Ax \rVert_{2}$, it equals to the largest singular value of $A$, that is $\sigma_{1}(A)$. For $A'=A$, we have $\lVert A \rVert_{2} = \rho(A)=\max |\lambda(A)|$
- 
    * **应用充分条件定理**: 我们找到了一个常数 $\alpha = 1 - c/C$，因为 $c>0$，所以 $\alpha < 1$；因为 $c \le C$，所以 $\alpha \ge 0$。故 $\alpha \in [0, 1)$。并且对于所有 $x$ 都有 $\|DG(x)\|_2 = \alpha \le \alpha$。
        根据上一节的定理（43.3），映射 $G(x) = x - \frac{1}{C} \nabla u(x)$ 是一个压缩映射，其压缩因子为 $\alpha = 1 - c/C$。
    * **应用巴拿赫不动点定理**: 由于 $G$ 是压缩映射，不动点迭代（即梯度下降）保证收敛到 $G$ 的唯一不动点，该不动点即为 $u$ 的全局最优解 $x^*$。证明（收敛性部分）完毕。

* **注意**: 如果问题的“条件数” $C/c$ 很大（意味着 $c$ 远小于 $C$），那么压缩因子 $\alpha = 1 - c/C$ 会非常接近 1，这将导致梯度下降法的**收敛速度很慢**（因为误差项按 $\alpha^k$ 衰减）。

# 44 牛顿法 (Newton's Method)

## 44.1 基本思想与算法

* 我们再次关注求解非线性方程组 $F(x) = 0$，其中 $F: \mathbb{R}^n \to \mathbb{R}^n$。
* **基本思想**：交替执行以下两个步骤：
    1.  在当前猜测解 $x^{(k)}$ 附近将方程**线性化 (Linearize)**。
    2.  **求解**这个线性化后的方程组，将解作为新的猜测解 $x^{(k+1)}$。
* **具体推导**:
    * 假设我们当前有一个猜测解 $x^{(k)}$。在 $x^{(k)}$ 附近，我们可以使用多元泰勒展开 (multivariate Taylor expansion) 将 $F(x)$ 展开到一阶（忽略高阶项）：
        $$ F(x) \approx F(x^{(k)}) + DF(x^{(k)}) (x - x^{(k)}) $$
        其中 $DF(x^{(k)})$ 是 $F$ 在 $x^{(k)}$ 处的**雅可比矩阵 (Jacobian matrix)**，即 $[DF(x^{(k)})]_{ij} = \frac{\partial f_i}{\partial x_j}(x^{(k)})$。
    * 我们用这个线性近似 $\tilde{F}(x) = F(x^{(k)}) + DF(x^{(k)}) (x - x^{(k)})$ 来代替原非线性方程 $F(x) = 0$ 中的 $F(x)$，求解 $\tilde{F}(x) = 0$，得到**线性化的方程组**：
        $$ 0 = F(x^{(k)}) + DF(x^{(k)}) (x - x^{(k)}) $$
    * 假设雅可比矩阵 $DF(x^{(k)})$ 在 $x^{(k)}$ 处可逆，我们可以解出 $x$：
        $$ DF(x^{(k)}) (x - x^{(k)}) = -F(x^{(k)}) $$
        $$ x - x^{(k)} = -[DF(x^{(k)})]^{-1} F(x^{(k)}) $$
        $$ x = x^{(k)} - [DF(x^{(k)})]^{-1} F(x^{(k)}) $$
* 我们将这个线性化方程组的解 $x$ 定义为**下一个猜测解 $x^{(k+1)}$**，从而确定了以下迭代公式，这被称为**牛顿法 (Newton's method)**：
    $$ x^{(k+1)} = x^{(k)} - [DF(x^{(k)})]^{-1} F(x^{(k)}) $$
    * 注意，在标量情况 ($n=1$) 下，$DF(x)$ 就是导数 $F'(x)$，牛顿法变为我们熟悉的形式：
        $$ x^{(k+1)} = x^{(k)} - \frac{F(x^{(k)})}{F'(x^{(k)})} $$
* **在优化问题中的应用**:
    * 如果我们要求解的是优化问题（例如最小化 $u(x)$），目标是找到临界点，即求解 $F(x) = \nabla u(x) = 0$。
    * 此时，函数 $F$ 的雅可比矩阵 $DF(x)$ 就是函数 $u$ 的**海森矩阵 (Hessian matrix)** $\nabla^2 u(x)$。
    * 因此，用于优化的牛顿法形式为：
        $$ x^{(k+1)} = x^{(k)} - [\nabla^2 u(x^{(k)})]^{-1} \nabla u(x^{(k)}) $$
    * 在标量情况 ($n=1$) 下，这变为：
        $$ x^{(k+1)} = x^{(k)} - \frac{u'(x^{(k)})}{u''(x^{(k)})} $$
* **实际计算**:
    * 在实践中，我们通常希望**避免直接计算雅可比矩阵的逆** $[DF(x^{(k)})]^{-1}$，因为求逆的计算量大（通常 $O(n^3)$）且可能数值不稳定。
    * 取而代之，我们将计算**牛顿步长 (Newton step)** $p_k = [DF(x^{(k)})]^{-1} F(x^{(k)})$ 的过程，视为求解一个**线性方程组**：
        $$ DF(x^{(k)}) p_k = F(x^{(k)}) $$
        其中 $x^{(k)}$ 是固定的，我们需要解出步长向量 $p_k$。
    * 然后更新解：$x^{(k+1)} = x^{(k)} - p_k$。

* **伪代码**:
    * **给定**: 初始猜测 $x^{(0)}$，最大迭代次数 $m_{max}$（或收敛判据 $\epsilon$）。
    * **初始化**: $x \leftarrow x^{(0)}$。
    * **循环**: 对于 $k = 0, 1, \dots, m_{max}-1$ （或直至满足收敛判据）：
        1. 计算当前点的雅可比矩阵 $J_k \leftarrow DF(x^{(k)})$。
        2. 计算当前点的函数值 $F_k \leftarrow F(x^{(k)})$。
        3. 求解线性方程组 $J_k p_k = F_k$ 得到**牛顿步长** $p_k$。
        4. 更新解：$x^{(k+1)} \leftarrow x^{(k)} - p_k$。
        5. (可选) 检查收敛性，例如 $\|p_k\| < \epsilon$ 或 $\|F(x^{(k+1)})\| < \epsilon$。如果满足则退出循环。
    * **输出**: 最终的近似解 $x^{(k+1)}$。



## 44.2 收敛阶 (Orders of Convergence)

* 我们想要讨论牛顿法的收敛性，但我们先稍微离题一下，来定义迭代方法**收敛阶 (order of convergence)** 的概念。
* **定义 (收敛阶)**：考虑一个迭代方法，给定初始猜测 $x^{(0)}$，产生序列 $\{x^{(k)}\}$，且 $\lim_{k\to\infty} x^{(k)} = x^*$。如果存在正整数 $q$ 和常数 $M > 0$ 使得对于所有（足够大的）$k$，都有
    $$ \frac{\|x^{(k+1)} - x^*\|}{\|x^{(k)} - x^*\|^q} \le M $$
    成立，那么该方法的**收敛阶**是 $q$，对应的**收敛率 (rate of convergence)** 是 $M$。
    * 等价地，我们可以将关键不等式写为：
        $$ \|x^{(k+1)} - x^*\| \le M \|x^{(k)} - x^*\|^q \quad \tag{*} $$
* $q=1$ 的情况称为**线性收敛 (linear convergence)**（我们之前在讨论不动点迭代时已经见过）。
    * 对于线性收敛 ($q=1$)，需要 $M < 1$ 才能保证收敛（无论初始猜测 $x^{(0)}$ 离 $x^*$ 多近）。因此，当 $q=1$ 时，$M < 1$ 通常是定义中一个隐含的要求。
* $q > 1$ 的情况称为**超线性收敛 (superlinear convergence)**。$q=2$ 的情况特别称为**二次收敛 (quadratic convergence)**。
    * 事实上，对于超线性收敛 ($q > 1$)，我们**不需要** $M < 1$ 来保证收敛。只要初始猜测**足够接近** $x^*$（即 $\|x^{(0)} - x^*\|$ 足够小），不等式 $(*)$ 本身就能保证收敛（误差会越来越小）。
    * 实践中，一个迭代方法可能开始时收敛较慢，直到迭代点进入收敛区域后才表现出超线性收敛。如果只对**渐近收敛行为 (asymptotic convergence behavior)** 感兴趣，那么假设初始猜测足够接近真实解是合理的。
* 下面的定理解释了为什么当 $q > 1$ 时，不等式 $(*)$ 意味着“极快的**局部收敛 (local convergence)**”。

**定理**. 假设一个迭代方法满足不等式 $(*)$ $\|x^{(k+1)} - x^*\| \le M \|x^{(k)} - x^*\|^q$，其中 $q > 1$。如果初始误差 $E_0 = \|x^{(0)} - x^*\|$ 满足 $E_0 \le \frac{1}{M^{1/(q-1)}}$*（原文条件，严格来说应为 $E_0 < M^{-1/(q-1)}$ 才能导出 $\alpha > 0$）*，那么该方法收敛到 $x^*$，并且存在常数 $\alpha, C > 0$ 使得误差界满足：
    $$ \|x^{(k)} - x^*\| \le C e^{-\alpha q^k} $$
*（这表明误差以 $q^k$ 指数速度的更快速度衰减）。*

**注记**. 事实上，我们可以取 $C = M^{-1/(q-1)}$，以及 $\alpha = -\log(M^{1/(q-1)} \|x^{(0)} - x^*\|)$。
* **证明概要**:
    * 定义第 $k$ 次迭代的误差为 $E_k := \|x^{(k)} - x^*\|$。则不等式 $(*)$ 变为 $E_{k+1} \le M E_k^q$。
    * 对不等式两边取对数得到 $\log E_{k+1} \le \log M + q \log E_k$。令 $b := \log M$，则 $\log E_{k+1} \le b + q \log E_k$。
    * 定义一个递归序列 $a_k$：$a_0 = \log E_0$，且 $a_{k+1} = b + q a_k$ 对于 $k=0, 1, 2, \dots$。
    * 可以通过归纳法证明这个序列 $a_k$ 构成了对数误差序列 $\log E_k$ 的一个上界，即 $\log E_k \le a_k$ 对所有 $k$ 成立。
        * （基础 $k=0$ 成立。假设 $\log E_k \le a_k$，则 $\log E_{k+1} \le b + q \log E_k \le b + q a_k = a_{k+1}$。归纳成立。）
    * 因此我们只需要对 $a_k$ 进行界定。$a_k$ 的显式表达式可以通过解这个线性递推关系得到：
        $$ a_k = q^k a_0 + b \sum_{j=0}^{k-1} q^j = q^k a_0 + b \frac{q^k - 1}{q - 1} = q^k \left( a_0 + \frac{b}{q-1} \right) - \frac{b}{q-1} $$
    * 我们希望当 $k \to \infty$ 时 $a_k \to -\infty$
	    * 这样 $\log E_k \to -\infty$，即 $E_k \to 0$。
	    * 由于 $q>1$，$q^k \to \infty$，为了使 $a_k \to -\infty$，我们需要 $q^k$ 的系数为负。
    * 因此需要 $a_0 + \frac{b}{q-1} < 0$。我们定义 $\alpha := -\left( a_0 + \frac{b}{q-1} \right)$，要求 $\alpha > 0$。
    * 回忆 $a_0 = \log E_0$ 和 $b = \log M$，条件 $a_0 + \frac{b}{q-1} < 0$ 变为 $\log E_0 + \frac{\log M}{q-1} < 0$，即 $\log(E_0 M^{1/(q-1)}) < 0$，
	    * 这等价于 $E_0 M^{1/(q-1)} < 1$，或 $E_0 < M^{-1/(q-1)}$。
	    * 这与定理的（严格形式）条件一致。
	    * 同时，$\alpha = -\log(E_0 M^{1/(q-1)})$ 也与注记中的定义相符。
    * 那么 $\log E_k \le a_k = -\alpha q^k - \frac{b}{q-1}$。
    * 两边取指数得到 $E_k \le e^{a_k} = e^{-\alpha q^k - b/(q-1)} = e^{-b/(q-1)} e^{-\alpha q^k}$。
    * 令 $C = e^{-b/(q-1)}$，则 $E_k \le C e^{-\alpha q^k}$。
    * 验证注记中的 $C$：$C = e^{-b/(q-1)} = e^{-(\log M)/(q-1)} = (e^{\log M})^{-1/(q-1)} = M^{-1/(q-1)}$。证明完毕。


## 44.3 牛顿法的局部收敛性

* 现在我们终于来研究牛顿法的收敛性。

**定理 (牛顿法的局部二次收敛)**.
假设函数 $F: \mathbb{R}^n \to \mathbb{R}^n$ 的所有分量函数 $f_i$ 都是**二次连续可微 ($C^2$)** 的。令 $x^*$ 是 $F(x) = 0$ 的一个解，并假设 $F$ 在 $x^*$ 处的雅可比矩阵 $DF(x^*)$ 是**可逆的 (invertible)**。那么，只要初始猜测 $x^{(0)}$ **足够接近** $x^*$，牛顿法产生的序列 $\{x^{(k)}\}$ 将以**二次收敛 (quadratically converges)** 的速度收敛到 $x^*$。

**注记**.
- 在标量情况 ($n=1$) 下，$DF(x^*)$ 可逆的假设相当于要求 $F'(x^*) \neq 0$。这是一个“**非退化 (nondegeneracy)**”假设（即解不是重根或临界点不是平坦的）。
- 在优化问题中（$F = \nabla u$），这个非退化假设意味着海森矩阵 $\nabla^2 u(x^*)$ 是正定的（所有特征值都为正）。
	- （如果情况不是这样，那么函数图像在最优点 $x^*$ 附近在某个方向上是“完全平坦”的，可能导致牛顿法失效或收敛性变差。）

* **证明**: 为简单起见，我们只证明标量情况 ($n=1$)。对于一般 $n$ 维情况的证明思路类似，但需要使用多元泰勒展开和矩阵范数。
    * **设置**:
        * 由于 $F'(x^*) \neq 0$ 且 $F'$ 连续（因为 $F \in C^2$ 意味着 $F' \in C^1$），根据连续函数的局部性质，存在某个邻域 $I := (x^* - \delta, x^* + \delta)$（其中 $\delta > 0$）以及某个常数 $\epsilon > 0$，使得对于所有位于该区间内的 $x$，都有 $|F'(x)| \ge \epsilon$。
        * 由于 $F$ 是二次连续可微的 ($F \in C^2$)，其二阶导数 $F''$ 在闭区间 $[x^* - \delta, x^* + \delta]$ 上连续，因此有界。即存在某个常数 $C$ 使得对于所有 $x \in I$，都有 $|F''(x)| \le C$。
        * 我们假设初始猜测 $x^{(0)}$ 足够接近 $x^*$，使得其初始误差 $E_0 = |x^{(0)} - x^*|$ 满足 $E_0 \le \min(\delta, \frac{\epsilon}{C})$。（令 $M := \frac{C}{2\epsilon}$，这个条件弱于 $E_0 \le \min(\delta, \frac{1}{M})$，但足以启动证明）。这保证了 $x^{(0)} \in I$。
    * **推导迭代误差关系**:
        * 假设当前迭代点 $x^{(k)} \in I$。
        * 对函数 $F$ 在点 $x^{(k)}$ 处进行泰勒展开，并在 $x^*$ 处取值，利用带有拉格朗日余项的泰勒公式：
            $$ F(x^*) = F(x^{(k)}) + F'(x^{(k)})(x^* - x^{(k)}) + \frac{1}{2} F''(\xi^{(k)})(x^* - x^{(k)})^2 $$
            其中 $\xi^{(k)}$ 是某个介于 $x^*$ 和 $x^{(k)}$ 之间的值（因此 $\xi^{(k)} \in I$）。
        * 由于 $F(x^*) = 0$，我们有：
            $$ 0 = F(x^{(k)}) + F'(x^{(k)})(x^* - x^{(k)}) + \frac{1}{2} F''(\xi^{(k)})(x^* - x^{(k)})^2 $$
        * 因为 $x^{(k)} \in I$，所以 $F'(x^{(k)}) \neq 0$（且 $|F'(x^{(k)})| \ge \epsilon > 0$），两边同除以 $F'(x^{(k)})$：
            $$ 0 = \frac{F(x^{(k)})}{F'(x^{(k)})} + (x^* - x^{(k)}) + \frac{F''(\xi^{(k)})}{2F'(x^{(k)})}(x^* - x^{(k)})^2 $$
        * 移项得到：
            $$ x^{(k)} - \frac{F(x^{(k)})}{F'(x^{(k)})} - x^* = - \frac{F''(\xi^{(k)})}{2F'(x^{(k)})}(x^* - x^{(k)})^2 $$
        * 左侧正好是牛顿法的下一次迭代 $x^{(k+1)}$ 减去真解 $x^*$：
            $$ x^{(k+1)} - x^* = - \frac{F''(\xi^{(k)})}{2F'(x^{(k)})}(x^{(k)} - x^*)^2 $$
    * **导出二次收敛不等式**:
        * 两边取绝对值：
            $$ |x^{(k+1)} - x^*| = \left| \frac{F''(\xi^{(k)})}{2F'(x^{(k)})} \right| |x^{(k)} - x^*|^2 $$
        * 利用之前得到的界限 $|F''(\xi^{(k)})| \le C$ 和 $|F'(x^{(k)})| \ge \epsilon$：
            $$ |x^{(k+1)} - x^*| \le \frac{C}{2\epsilon} |x^{(k)} - x^*|^2 $$
        * 令 $M = \frac{C}{2\epsilon}$（这是一个正常数），则：
            $$ |x^{(k+1)} - x^*| \le M |x^{(k)} - x^*|^2 $$
    * **收敛性结论**:
        * 这个不等式表明误差 $E_{k+1} = |x^{(k+1)} - x^*|$ 与前一步误差的平方 $E_k^2 = |x^{(k)} - x^*|^2$ 成正比。这正是**二次收敛 ($q=2$)** 的定义。
        * 根据上一节关于超线性收敛的定理，只要初始误差 $E_0$ 足够小（例如 $E_0 < 1/M = 2\epsilon/C$），并且也满足 $E_0 \le \delta$ 使得迭代保持在区间 $I$ 内（实际上 $E_{k+1} \le M E_k^2 = (M E_k) E_k \le (M E_0) E_k \le E_k$，误差单调递减，因此如果 $E_0 \le \delta$，则所有 $E_k \le \delta$），那么序列 $x^{(k)}$ 就会收敛到 $x^*$，并且收敛阶为二次。

* **证明总结**: 通过对 $F(x^*)$ 在 $x^{(k)}$ 处进行泰勒展开，并利用 $F(x^*)=0$ 和牛顿法迭代公式 $x^{(k+1)} = x^{(k)} - F(x^{(k)})/F'(x^{(k)})$，我们导出了误差关系 $|x^{(k+1)} - x^*| \le M |x^{(k)} - x^*|^2$。这表明在 $DF(x^*)$ 可逆且 $F$ 足够光滑的条件下，只要初始猜测足够接近解 $x^*$，牛顿法就具有局部二次收敛性。